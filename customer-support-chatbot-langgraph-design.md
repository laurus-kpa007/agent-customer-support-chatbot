# ê³ ê°ì§€ì› ì±—ë´‡ Agent - LangGraph PoC ìƒì„¸ ì„¤ê³„

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [PoC ë²”ìœ„ ë° ì œì•½ì‚¬í•­](#poc-ë²”ìœ„-ë°-ì œì•½ì‚¬í•­)
3. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
4. [ë°ì´í„° ëª¨ë¸](#ë°ì´í„°-ëª¨ë¸)
5. [LangGraph ì›Œí¬í”Œë¡œìš° ì„¤ê³„](#langgraph-ì›Œí¬í”Œë¡œìš°-ì„¤ê³„)
6. [ì£¼ìš” ë…¸ë“œ ìƒì„¸ ì„¤ê³„](#ì£¼ìš”-ë…¸ë“œ-ìƒì„¸-ì„¤ê³„)
7. [Human-in-the-Loop êµ¬í˜„](#human-in-the-loop-êµ¬í˜„)
8. [ê¸°ìˆ  ìŠ¤íƒ](#ê¸°ìˆ -ìŠ¤íƒ)
9. [ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ë° ì²­í‚¹ ì „ëµ](#ë²¡í„°-ìŠ¤í† ì–´-êµ¬ì¶•-ë°-ì²­í‚¹-ì „ëµ) â­ NEW
10. [ë””ë ‰í† ë¦¬ êµ¬ì¡°](#ë””ë ‰í† ë¦¬-êµ¬ì¡°)
11. [êµ¬í˜„ ë‹¨ê³„](#êµ¬í˜„-ë‹¨ê³„)

---

## ê°œìš”

### í”„ë¡œì íŠ¸ ëª©í‘œ
FAQì™€ Q&A ê²Œì‹œíŒ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆì˜ì— ë‹¨ê³„ë³„(Step-by-Step)ë¡œ ë‹µë³€í•˜ê³ , í•´ê²°ë˜ì§€ ì•Šì„ ê²½ìš° ìë™ìœ¼ë¡œ ê²Œì‹œíŒì— ë“±ë¡í•˜ëŠ” ê³ ê°ì§€ì› ì±—ë´‡ PoC êµ¬ì¶•

### í•µì‹¬ ê¸°ëŠ¥
1. âœ… FAQ/Q&A ë°ì´í„° ë²¡í„°í™” ë° ê²€ìƒ‰
2. âœ… ë‹¨ê³„ë³„ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ ì œê³µ
3. âœ… Human-in-the-Loop ë°©ì‹ ëŒ€í™” ì§„í–‰
4. âœ… ë¯¸í•´ê²° ì‹œ ìë™ í‹°ì¼“ ìƒì„±
5. âœ… ë‹µë³€ ë“±ë¡ ì‹œ ì•Œë¦¼ ë°œì†¡

---

## PoC ë²”ìœ„ ë° ì œì•½ì‚¬í•­

### PoCì— í¬í•¨ë˜ëŠ” ê¸°ëŠ¥
- âœ… ë¡œì»¬ LLM ê¸°ë°˜ RAG (Ollama Gemma3 27b)
- âœ… í•œê¸€ ìµœì í™” ì„ë² ë”© (Ollama BGE-M3-Korean)
- âœ… Chroma ë²¡í„° ìŠ¤í† ì–´ (1000ê°œ FAQ ì§€ì›)
- âœ… LangGraph StateGraphë¥¼ ì´ìš©í•œ ì›Œí¬í”Œë¡œìš°
- âœ… FAQ êµ¬ì¡°í™”ëœ ë°ì´í„° (ì¦ìƒ/ì›ì¸/ì„ì‹œì¡°ì¹˜)
- âœ… ë‹¨ê³„ë³„ ë‹µë³€ ì œê³µ (ìµœëŒ€ 3ë‹¨ê³„)
- âœ… Human-in-the-Loop ì¸í„°ëŸ½íŠ¸
- âœ… ì‹¤ì‹œê°„ ì§„í–‰ ìƒíƒœ í‘œì‹œ
- âœ… **Streamlit WebUI** (ì±„íŒ… ì¸í„°í˜ì´ìŠ¤)
- âœ… ê°„ë‹¨í•œ í‹°ì¼“ ìƒì„± (JSON íŒŒì¼ ì €ì¥)
- âœ… ì´ë©”ì¼ ì•Œë¦¼ (ì½˜ì†” ì¶œë ¥ ì‹œë®¬ë ˆì´ì…˜)

### PoCì—ì„œ ì œì™¸ë˜ëŠ” ê¸°ëŠ¥
- âŒ ì‹¤ì œ ì›¹ í¬ë¡¤ë§ (1000ê°œ ìƒ˜í”Œ FAQ ë°ì´í„° ì‚¬ìš©)
- âŒ ë³µì¡í•œ GraphRAG (ê°„ë‹¨í•œ ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´)
- âŒ í”„ë¡œë•ì…˜ ë°ì´í„°ë² ì´ìŠ¤ (SQLite ì‚¬ìš©)
- âŒ ì‹¤ì œ ê²Œì‹œíŒ API ì—°ë™ (Mock êµ¬í˜„)
- âŒ ì¸ì¦/ë³´ì•ˆ ê¸°ëŠ¥

### ì„±ê³µ ê¸°ì¤€
1. ì‚¬ìš©ì ì§ˆì˜ì— ëŒ€í•´ ê´€ë ¨ FAQ ê²€ìƒ‰ ì„±ê³µë¥  > 70%
2. 3ë‹¨ê³„ ì´ë‚´ ë‹¨ê³„ë³„ ë‹µë³€ ì œê³µ
3. ëŒ€í™” íˆìŠ¤í† ë¦¬ ìœ ì§€ ë° ì»¨í…ìŠ¤íŠ¸ ì´í•´
4. ë¯¸í•´ê²° ì‹œ í‹°ì¼“ ìë™ ìƒì„±
5. ì „ì²´ ì›Œí¬í”Œë¡œìš° ì •ìƒ ë™ì‘

---

## ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ì „ì²´ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TB
    subgraph UserInterface["ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤"]
        WebUI[Streamlit WebUI<br/>ì±„íŒ… ì¸í„°í˜ì´ìŠ¤]
        StatusDisplay[ì‹¤ì‹œê°„ ìƒíƒœ í‘œì‹œ<br/>Progress Bar]
    end

    subgraph LangGraphLayer["LangGraph ì›Œí¬í”Œë¡œìš° ë ˆì´ì–´"]
        StateGraph[StateGraph<br/>ìƒíƒœ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°]

        subgraph Nodes["ë…¸ë“œë“¤"]
            InitNode[ì´ˆê¸°í™” ë…¸ë“œ<br/>initialize]
            ClassifyNode[ì˜ë„ ë¶„ë¥˜ ë…¸ë“œ<br/>classify_intent]
            SmallTalkNode[ìŠ¤ëª°í†¡ ì²˜ë¦¬ ë…¸ë“œ<br/>handle_small_talk]
            RAGNode[RAG ê²€ìƒ‰ ë…¸ë“œ<br/>search_knowledge]
            PlanNode[ë‹µë³€ ê³„íš ë…¸ë“œ<br/>plan_response]
            RespondNode[ì‘ë‹µ ë…¸ë“œ<br/>respond_step]
            EvalNode[í‰ê°€ ë…¸ë“œ<br/>evaluate_status]
            ConfirmNode[í‹°ì¼“ í™•ì¸ ë…¸ë“œ<br/>confirm_ticket]
            EvalTicketNode[í‹°ì¼“ ì‘ë‹µ í‰ê°€ ë…¸ë“œ<br/>evaluate_ticket_confirmation]
            TicketNode[í‹°ì¼“ ìƒì„± ë…¸ë“œ<br/>create_ticket]
            NotifyNode[ì•Œë¦¼ ë…¸ë“œ<br/>send_notification]
        end

        Memory[Checkpointer<br/>SqliteSaver]
        StatusUpdater[ìƒíƒœ ì—…ë°ì´íŠ¸<br/>ê° ë…¸ë“œë§ˆë‹¤]
    end

    subgraph DataLayer["ë°ì´í„° ë ˆì´ì–´"]
        VectorDB[(Vector Store<br/>Chroma)]
        TicketDB[(í‹°ì¼“ DB<br/>SQLite)]
        SampleData[1000ê°œ FAQ<br/>JSON êµ¬ì¡°í™”]
    end

    subgraph LLMLayer["ë¡œì»¬ LLM ë ˆì´ì–´"]
        LLM[Ollama<br/>Gemma3 27b]
        Embeddings[Ollama<br/>BGE-M3-Korean]
    end

    subgraph ExternalMock["ì™¸ë¶€ ì‹œìŠ¤í…œ Mock"]
        EmailMock[ì´ë©”ì¼ ë°œì†¡<br/>Console Log]
        QABoardMock[Q&A ê²Œì‹œíŒ<br/>JSON File]
    end

    WebUI --> StateGraph
    StateGraph --> Nodes
    StateGraph --> Memory
    Nodes --> StatusUpdater
    StatusUpdater --> StatusDisplay

    Nodes --> LLM
    Nodes --> Embeddings
    Nodes --> VectorDB
    Nodes --> TicketDB
    Nodes --> EmailMock
    Nodes --> QABoardMock

    SampleData --> VectorDB

    style UserInterface fill:#e1f5ff
    style LangGraphLayer fill:#e8f5e9
    style DataLayer fill:#fff3e0
    style LLMLayer fill:#fce4ec
    style ExternalMock fill:#f3e5f5
```

### LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„¸ë„

```mermaid
graph TD
    START([ì‚¬ìš©ì ì…ë ¥]) --> Initialize[ì´ˆê¸°í™”<br/>initialize]

    Initialize --> SearchKnowledge[ì§€ì‹ ê²€ìƒ‰<br/>search_knowledge<br/>Vector Search]

    SearchKnowledge --> PlanResponse[ë‹µë³€ ê³„íš<br/>plan_response<br/>ë‹¨ê³„ë³„ ë‹µë³€ ì¤€ë¹„]

    PlanResponse --> RespondStep[ë‹¨ê³„ ì‘ë‹µ<br/>respond_step<br/>í˜„ì¬ ë‹¨ê³„ ë‹µë³€]

    RespondStep --> |interrupt| HumanInput{{ì‚¬ìš©ì ì‘ë‹µ ëŒ€ê¸°<br/>Human-in-the-Loop}}

    HumanInput --> EvaluateStatus[ìƒíƒœ í‰ê°€<br/>evaluate_status]

    EvaluateStatus --> |í•´ê²°ë¨| Resolved([ëŒ€í™” ì¢…ë£Œ<br/>ë¬¸ì œ í•´ê²°])

    EvaluateStatus --> |ë‹¤ìŒ ë‹¨ê³„| NextStepCheck{ë‚¨ì€ ë‹¨ê³„<br/>ìˆëŠ”ê°€?}

    NextStepCheck --> |ì˜ˆ| RespondStep
    NextStepCheck --> |ì•„ë‹ˆì˜¤| CreateTicket[í‹°ì¼“ ìƒì„±<br/>create_ticket<br/>ê²Œì‹œíŒ ë“±ë¡]

    EvaluateStatus --> |ì—ìŠ¤ì»¬ë ˆì´ì…˜| CreateTicket

    CreateTicket --> SendNotification[ì•Œë¦¼ ë°œì†¡<br/>send_notification<br/>ì´ë©”ì¼/í‘¸ì‹œ]

    SendNotification --> END([ì›Œí¬í”Œë¡œìš° ì¢…ë£Œ])

    style START fill:#4CAF50,color:#fff
    style Initialize fill:#2196F3,color:#fff
    style SearchKnowledge fill:#FF9800,color:#fff
    style PlanResponse fill:#9C27B0,color:#fff
    style RespondStep fill:#00BCD4,color:#fff
    style HumanInput fill:#F44336,color:#fff
    style EvaluateStatus fill:#FFEB3B
    style CreateTicket fill:#E91E63,color:#fff
    style SendNotification fill:#3F51B5,color:#fff
    style Resolved fill:#4CAF50,color:#fff
    style END fill:#607D8B,color:#fff
```

---

## ë°ì´í„° ëª¨ë¸

### 1. State ê°ì²´ (TypedDict)

```python
from typing import TypedDict, List, Dict, Literal, Optional
from langchain_core.messages import BaseMessage

class SupportState(TypedDict):
    """ê³ ê°ì§€ì› ì±—ë´‡ì˜ ì „ì²´ ìƒíƒœ"""

    # ëŒ€í™” ê´€ë ¨
    messages: List[BaseMessage]              # ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬
    current_query: str                       # í˜„ì¬ ì‚¬ìš©ì ì§ˆì˜

    # RAG ê²€ìƒ‰ ê²°ê³¼
    retrieved_docs: List[Dict]               # ê²€ìƒ‰ëœ FAQ ë¬¸ì„œë“¤
    relevance_score: float                   # ê´€ë ¨ì„± ì ìˆ˜

    # ë‹¨ê³„ë³„ ë‹µë³€ ê³„íš
    solution_steps: List[Dict]               # í•´ê²° ë‹¨ê³„ ëª©ë¡
    # ì˜ˆ: [
    #   {"step": 1, "action": "...", "description": "...", "completed": False},
    #   {"step": 2, "action": "...", "description": "...", "completed": False}
    # ]
    current_step: int                        # í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ë‹¨ê³„ (0ë¶€í„° ì‹œì‘)
    max_steps: int                           # ìµœëŒ€ ë‹¨ê³„ ìˆ˜ (ê¸°ë³¸ 3)

    # ìƒíƒœ ì¶”ì 
    status: Literal[
        "initialized",        # ì´ˆê¸°í™”ë¨
        "searching",          # ê²€ìƒ‰ ì¤‘
        "small_talking",      # ìŠ¤ëª°í†¡ ì¤‘
        "planning",           # ë‹µë³€ ê³„íš ì¤‘
        "responding",         # ì‘ë‹µ ì¤‘
        "waiting_user",       # ì‚¬ìš©ì ì‘ë‹µ ëŒ€ê¸°
        "evaluating",         # í‰ê°€ ì¤‘
        "resolved",           # í•´ê²°ë¨
        "escalated",          # ì—ìŠ¤ì»¬ë ˆì´ì…˜
        "confirming_ticket",  # í‹°ì¼“ í™•ì¸ ì¤‘
        "evaluating_ticket",  # í‹°ì¼“ ì‘ë‹µ í‰ê°€ ì¤‘
        "ticket_created",     # í‹°ì¼“ ìƒì„±ë¨
        "cancelled"           # í‹°ì¼“ ì·¨ì†Œë¨
    ]

    # ì—ìŠ¤ì»¬ë ˆì´ì…˜ ê´€ë ¨
    attempts: int                            # ì‹œë„ íšŸìˆ˜
    unresolved_reason: Optional[str]         # ë¯¸í•´ê²° ì‚¬ìœ 
    ticket_id: Optional[str]                 # ìƒì„±ëœ í‹°ì¼“ ID
    ticket_confirmed: Optional[bool]         # í‹°ì¼“ ìƒì„± í™•ì¸ (True: ìƒì„±, False: ì·¨ì†Œ, None: ë¯¸ì •)

    # ì˜ë„ ë¶„ë¥˜
    intent: Optional[Literal["small_talk", "technical_support", "continue_conversation"]]  # ì‚¬ìš©ì ì˜ë„
    intent_confidence: Optional[float]       # ì˜ë„ ë¶„ë¥˜ ì‹ ë¢°ë„

    # ë©”íƒ€ë°ì´í„°
    user_id: str                             # ì‚¬ìš©ì ID
    session_id: str                          # ì„¸ì…˜ ID
    started_at: str                          # ì‹œì‘ ì‹œê°„

    # ë””ë²„ê·¸ ì •ë³´
    debug_info: Optional[Dict]               # ë””ë²„ê·¸ ì •ë³´
```

### 2. FAQ ë¬¸ì„œ êµ¬ì¡°

```python
class FAQSolution(TypedDict):
    """ê°œë³„ í•´ê²° ë°©ë²• êµ¬ì¡°"""
    method: int                              # ë°©ë²• ë²ˆí˜¸ (1, 2, 3, ...)
    title: str                               # ë°©ë²• ì œëª©
    steps: List[str]                         # ì‹¤í–‰ ë‹¨ê³„ë“¤
    expected_result: str                     # ê¸°ëŒ€ë˜ëŠ” ê²°ê³¼

class FAQContent(TypedDict):
    """FAQ ë³¸ë¬¸ êµ¬ì¡° (ì¦ìƒ/ì›ì¸/ì„ì‹œì¡°ì¹˜)"""
    symptom: str                             # ì¦ìƒ ì„¤ëª…
    cause: str                               # ì›ì¸ ì„¤ëª…
    solutions: List[FAQSolution]             # ì„ì‹œì¡°ì¹˜ ë°©ë²•ë“¤ (ë°©ë²•1, ë°©ë²•2, ...)

class FAQDocument(TypedDict):
    """FAQ ë¬¸ì„œ êµ¬ì¡° (1000ê°œ ê²Œì‹œê¸€ ê¸°ì¤€)"""
    id: str                                  # ë¬¸ì„œ ID (ì˜ˆ: FAQ-001)
    category: str                            # ì¹´í…Œê³ ë¦¬ (ë©”ì‹ ì €, ë¡œê·¸ì¸, ì•Œë¦¼ ë“±)
    title: str                               # ê²Œì‹œê¸€ ì œëª©
    content: FAQContent                      # ë³¸ë¬¸ (ì¦ìƒ/ì›ì¸/ì„ì‹œì¡°ì¹˜)
    tags: List[str]                          # íƒœê·¸
    created_at: str                          # ìƒì„±ì¼
    updated_at: str                          # ìˆ˜ì •ì¼
    view_count: int                          # ì¡°íšŒìˆ˜
    helpful_count: int                       # ë„ì›€ë¨ ìˆ˜
    source: Literal["faq", "qa_board"]       # ì¶œì²˜

# ì˜ˆì‹œ ë°ì´í„°
example_faq = {
    "id": "FAQ-001",
    "category": "ë©”ì‹ ì €",
    "title": "ì‹ ì°© ë©”ì‹œì§€ ì•Œë¦¼ì´ í‘œì‹œë˜ì§€ ì•ŠìŒ",
    "content": {
        "symptom": "ë©”ì‹ ì €ì—ì„œ ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ë°›ì•„ë„ ì•Œë¦¼ì°½ì´ ëœ¨ì§€ ì•ŠìŠµë‹ˆë‹¤.",
        "cause": "ì•Œë¦¼ ì„¤ì •ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆê±°ë‚˜, ìš´ì˜ì²´ì œì˜ ì•Œë¦¼ ê¶Œí•œì´ ê±°ë¶€ëœ ê²½ìš° ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "solutions": [
            {
                "method": 1,
                "title": "ë©”ì‹ ì € ì•Œë¦¼ ì„¤ì • í™•ì¸",
                "steps": [
                    "í™˜ê²½ì„¤ì • ë©”ë‰´ë¥¼ ì—½ë‹ˆë‹¤",
                    "ì•Œë¦¼ íƒ­ì„ ì„ íƒí•©ë‹ˆë‹¤",
                    "'ì•Œë¦¼ì°½' ì˜µì…˜ì— ì²´í¬ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤"
                ],
                "expected_result": "ì•Œë¦¼ì°½ì— ì²´í¬ê°€ ë˜ì–´ìˆì–´ì•¼ í•©ë‹ˆë‹¤"
            },
            {
                "method": 2,
                "title": "ìœˆë„ìš° ì•Œë¦¼ ì„¤ì • í™•ì¸",
                "steps": [
                    "ìœˆë„ìš° ì‹œì‘ ë©”ë‰´ë¥¼ ì—½ë‹ˆë‹¤",
                    "ì„¤ì • > ì‹œìŠ¤í…œ > ì•Œë¦¼ ë° ì‘ì—…ì„ ì„ íƒí•©ë‹ˆë‹¤",
                    "'ì•± ë° ë‹¤ë¥¸ ë³´ë‚¸ ì‚¬ëŒì˜ ì•Œë¦¼ ë°›ê¸°'ë¥¼ ì¼œì§ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤"
                ],
                "expected_result": "ëª¨ë“  ì•Œë¦¼ ì„¤ì •ì´ ì¼œì§ ìƒíƒœì—¬ì•¼ í•©ë‹ˆë‹¤"
            },
            {
                "method": 3,
                "title": "ë©”ì‹ ì € ì¬ì‹œì‘",
                "steps": [
                    "ì‘ì—… í‘œì‹œì¤„ì—ì„œ ë©”ì‹ ì € ì•„ì´ì½˜ì„ ìš°í´ë¦­í•©ë‹ˆë‹¤",
                    "'ì¢…ë£Œ'ë¥¼ ì„ íƒí•©ë‹ˆë‹¤",
                    "ë©”ì‹ ì €ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•©ë‹ˆë‹¤"
                ],
                "expected_result": "ì¬ì‹œì‘ í›„ ì•Œë¦¼ì´ ì •ìƒì ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤"
            }
        ]
    },
    "tags": ["ì•Œë¦¼", "ë©”ì‹ ì €", "ì„¤ì •"],
    "created_at": "2023-08-15",
    "updated_at": "2023-11-10",
    "view_count": 1247,
    "helpful_count": 982,
    "source": "faq"
}
```

### 3. í‹°ì¼“ êµ¬ì¡°

```python
class Ticket(TypedDict):
    """Q&A ê²Œì‹œíŒ í‹°ì¼“ êµ¬ì¡°"""
    ticket_id: str                           # í‹°ì¼“ ID
    user_id: str                             # ì‚¬ìš©ì ID
    title: str                               # ì œëª©
    content: str                             # ë‚´ìš© (ëŒ€í™” ìš”ì•½)
    conversation_history: List[Dict]         # ì „ì²´ ëŒ€í™” ë‚´ì—­
    category: str                            # ì¹´í…Œê³ ë¦¬
    status: Literal["open", "answered", "closed"]
    created_at: str                          # ìƒì„± ì‹œê°„
    answered_at: Optional[str]               # ë‹µë³€ ì‹œê°„
    answer: Optional[str]                    # ë‹µë³€ ë‚´ìš©
```

---

## LangGraph ì›Œí¬í”Œë¡œìš° ì„¤ê³„

### StateGraph êµ¬ì¡°

```python
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.sqlite import SqliteSaver

# ì²´í¬í¬ì¸í„° ìƒì„± (ëŒ€í™” ìƒíƒœ ì €ì¥)
memory = SqliteSaver.from_conn_string("checkpoints.db")

# StateGraph ìƒì„±
workflow = StateGraph(SupportState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("initialize", initialize_node)
workflow.add_node("classify_intent", classify_intent_node)
workflow.add_node("handle_small_talk", handle_small_talk_node)
workflow.add_node("search_knowledge", search_knowledge_node)
workflow.add_node("plan_response", plan_response_node)
workflow.add_node("respond_step", respond_step_node)
workflow.add_node("evaluate_status", evaluate_status_node)
workflow.add_node("confirm_ticket", confirm_ticket_node)
workflow.add_node("evaluate_ticket_confirmation", evaluate_ticket_confirmation_node)
workflow.add_node("create_ticket", create_ticket_node)
workflow.add_node("send_notification", send_notification_node)

# ì—£ì§€ ì •ì˜
workflow.set_entry_point("initialize")

# initialize ì´í›„ ì¡°ê±´ë¶€ ë¼ìš°íŒ…
workflow.add_conditional_edges(
    "initialize",
    route_after_initialize,
    {
        "evaluate_ticket": "evaluate_ticket_confirmation",  # í‹°ì¼“ í™•ì¸ í‰ê°€
        "classify": "classify_intent",                      # ì˜ë„ ë¶„ë¥˜
        "evaluate": "evaluate_status"                       # ëŒ€í™” í‰ê°€
    }
)

# classify_intent ì´í›„ ì¡°ê±´ë¶€ ë¼ìš°íŒ…
workflow.add_conditional_edges(
    "classify_intent",
    route_after_classify,
    {
        "small_talk": "handle_small_talk",      # ìŠ¤ëª°í†¡
        "search": "search_knowledge",            # ê¸°ìˆ  ì§€ì› - ê²€ìƒ‰
        "evaluate": "evaluate_status"            # ëŒ€í™” ê³„ì† - í‰ê°€
    }
)

# ìŠ¤ëª°í†¡ í›„ ì‚¬ìš©ì ëŒ€ê¸°
workflow.add_edge("handle_small_talk", END)

workflow.add_edge("search_knowledge", "plan_response")
workflow.add_edge("plan_response", "respond_step")

# respond_step í›„ ì‚¬ìš©ì ì‘ë‹µ ëŒ€ê¸° (Human-in-the-Loop)
workflow.add_edge("respond_step", END)

# evaluate_status ì´í›„ ì¡°ê±´ë¶€ ë¼ìš°íŒ…
workflow.add_conditional_edges(
    "evaluate_status",
    route_after_evaluate,
    {
        "continue": "respond_step",         # ë‹¤ìŒ ë‹¨ê³„ ê³„ì†
        "resolved": END,                    # í•´ê²° ì™„ë£Œ
        "confirm_ticket": "confirm_ticket"  # í‹°ì¼“ í™•ì¸
    }
)

# í‹°ì¼“ í™•ì¸ í›„ ì‚¬ìš©ì ëŒ€ê¸°
workflow.add_edge("confirm_ticket", END)

# í‹°ì¼“ í™•ì¸ í‰ê°€ í›„ ì¡°ê±´ë¶€ ë¼ìš°íŒ…
workflow.add_conditional_edges(
    "evaluate_ticket_confirmation",
    route_after_ticket_confirmation,
    {
        "create": "create_ticket",     # í‹°ì¼“ ìƒì„±
        "cancelled": END,               # ì·¨ì†Œ
        "wait": END                     # ì¬í™•ì¸ ëŒ€ê¸°
    }
)

workflow.add_edge("create_ticket", "send_notification")
workflow.add_edge("send_notification", END)

# ì»´íŒŒì¼
app = workflow.compile(checkpointer=memory)
```

### ë¼ìš°íŒ… ë¡œì§

```python
def route_next_action(state: SupportState) -> str:
    """ë‹¤ìŒ ì•¡ì…˜ ê²°ì •"""

    # í•´ê²°ë¨ìœ¼ë¡œ í‘œì‹œëœ ê²½ìš°
    if state["status"] == "resolved":
        return "resolved"

    # ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ í‹°ì¼“ ìš”ì²­
    last_message = state["messages"][-1].content.lower()
    if any(keyword in last_message for keyword in ["ë“±ë¡í•´", "ë¬¸ì˜í•´", "í‹°ì¼“"]):
        return "escalate"

    # ëª¨ë“  ë‹¨ê³„ë¥¼ ì‹œë„í–ˆëŠ”ë°ë„ í•´ê²° ì•ˆë¨
    if state["current_step"] >= len(state["solution_steps"]):
        return "escalate"

    # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
    if state["attempts"] >= 5:
        return "escalate"

    # ë‹¤ìŒ ë‹¨ê³„ ê³„ì†
    return "continue"
```

---

## ì£¼ìš” ë…¸ë“œ ìƒì„¸ ì„¤ê³„

### 1. Initialize Node (ì´ˆê¸°í™”)

```python
from datetime import datetime
import uuid

def initialize_node(state: SupportState) -> SupportState:
    """
    ëŒ€í™” ì´ˆê¸°í™” ë…¸ë“œ
    - ì„¸ì…˜ ì •ë³´ ì„¤ì •
    - ì´ˆê¸° ìƒíƒœ ì„¤ì •
    """

    # ì²« ì‹¤í–‰ì‹œì—ë§Œ ì´ˆê¸°í™”
    if "session_id" not in state or not state["session_id"]:
        state["session_id"] = str(uuid.uuid4())
        state["started_at"] = datetime.now().isoformat()
        state["attempts"] = 0
        state["current_step"] = 0
        state["max_steps"] = 3
        state["status"] = "initialized"

    # í˜„ì¬ ì¿¼ë¦¬ ì¶”ì¶œ (ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€)
    if state["messages"]:
        last_msg = state["messages"][-1]
        if last_msg.type == "human":
            state["current_query"] = last_msg.content

    state["attempts"] += 1
    state["status"] = "searching"

    return state
```

### 2. Search Knowledge Node (ì§€ì‹ ê²€ìƒ‰)

```python
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_core.documents import Document

def search_knowledge_node(state: SupportState) -> SupportState:
    """
    RAG ê²€ìƒ‰ ë…¸ë“œ
    - ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê´€ë ¨ FAQ ê²€ìƒ‰
    - ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
    """

    # ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
    embeddings = OllamaEmbeddings(model="bge-m3-korean")
    vectorstore = Chroma(
        persist_directory="data/vectorstore",
        embedding_function=embeddings
    )

    # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (ìƒìœ„ 3ê°œ)
    query = state["current_query"]
    docs_with_scores = vectorstore.similarity_search_with_score(
        query,
        k=3
    )

    # ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
    retrieved_docs = []
    for doc, score in docs_with_scores:
        retrieved_docs.append({
            "id": doc.metadata.get("id", ""),
            "category": doc.metadata.get("category", ""),
            "question": doc.metadata.get("question", ""),
            "answer": doc.page_content,
            "steps": doc.metadata.get("steps", []),
            "score": float(score),
            "source": doc.metadata.get("source", "faq")
        })

    state["retrieved_docs"] = retrieved_docs

    # ìµœê³  ì ìˆ˜ ì €ì¥ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ - ì½”ì‚¬ì¸ ê±°ë¦¬)
    state["relevance_score"] = docs_with_scores[0][1] if docs_with_scores else 1.0
    state["status"] = "planning"

    return state
```

### 3. Plan Response Node (ë‹µë³€ ê³„íš)

```python
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
import json

def plan_response_node(state: SupportState) -> SupportState:
    """
    ë‹µë³€ ê³„íš ë…¸ë“œ
    - ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¨ê³„ë³„ í•´ê²° ë°©ë²• ìƒì„±
    - LLMì„ í™œìš©í•œ ê³„íš ìˆ˜ë¦½
    """

    llm = ChatOllama(model="gemma2:27b", temperature=0)

    # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ í¬ë§·íŒ…
    docs_context = "\n\n".join([
        f"[ë¬¸ì„œ {i+1}] (ê´€ë ¨ë„: {doc['score']:.3f})\n"
        f"ì§ˆë¬¸: {doc['question']}\n"
        f"ë‹µë³€: {doc['answer']}\n"
        f"ë‹¨ê³„: {json.dumps(doc.get('steps', []), ensure_ascii=False)}"
        for i, doc in enumerate(state["retrieved_docs"])
    ])

    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ê³ ê°ì§€ì› ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë‹¨ê³„ë³„ ê°€ì´ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ:
{docs_context}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "steps": [
    {{
      "step": 1,
      "action": "í™•ì¸í•  í•­ëª© ë˜ëŠ” ìˆ˜í–‰í•  ì‘ì—…",
      "description": "ìƒì„¸ ì„¤ëª…",
      "expected_result": "ê¸°ëŒ€ë˜ëŠ” ê²°ê³¼"
    }},
    ...
  ],
  "estimated_difficulty": "easy|medium|hard"
}}

ìµœëŒ€ 3ë‹¨ê³„ê¹Œì§€ë§Œ ì‘ì„±í•˜ì„¸ìš”."""),
        ("user", "ì‚¬ìš©ì ë¬¸ì œ: {query}")
    ])

    # LLM í˜¸ì¶œ
    response = llm.invoke(
        prompt.format_messages(
            docs_context=docs_context,
            query=state["current_query"]
        )
    )

    # JSON íŒŒì‹±
    try:
        plan = json.loads(response.content)
        state["solution_steps"] = plan["steps"]

        # ê° ë‹¨ê³„ì— ì™„ë£Œ ì—¬ë¶€ ì¶”ê°€
        for step in state["solution_steps"]:
            step["completed"] = False

    except json.JSONDecodeError:
        # íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ë‹¨ê³„ ìƒì„±
        state["solution_steps"] = [{
            "step": 1,
            "action": "ê¸°ë³¸ í™•ì¸ ì‚¬í•­",
            "description": state["retrieved_docs"][0]["answer"] if state["retrieved_docs"] else "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "expected_result": "ë¬¸ì œ í•´ê²°",
            "completed": False
        }]

    state["current_step"] = 0
    state["status"] = "responding"

    return state
```

### 4. Respond Step Node (ë‹¨ê³„ë³„ ì‘ë‹µ)

```python
from langchain_core.messages import AIMessage

def respond_step_node(state: SupportState) -> SupportState:
    """
    í˜„ì¬ ë‹¨ê³„ì˜ ë‹µë³€ ì œê³µ
    - ì‚¬ìš©ìì—ê²Œ í˜„ì¬ ë‹¨ê³„ ì•ˆë‚´
    - Human-in-the-Loopì„ ìœ„í•œ ì‘ë‹µ ìƒì„±
    """

    current_idx = state["current_step"]
    steps = state["solution_steps"]

    # í˜„ì¬ ë‹¨ê³„ê°€ ì—†ìœ¼ë©´ ì—ìŠ¤ì»¬ë ˆì´ì…˜
    if current_idx >= len(steps):
        state["status"] = "escalated"
        state["unresolved_reason"] = "ëª¨ë“  ë‹¨ê³„ë¥¼ ì‹œë„í–ˆìœ¼ë‚˜ í•´ê²°ë˜ì§€ ì•ŠìŒ"

        response_text = (
            "ë¶ˆí¸ì„ ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤. ë‹´ë‹¹ ë¶€ì„œì˜ í™•ì¸ì´ í•„ìš”í•œ ìƒí™©ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n"
            "í˜„ì¬ê¹Œì§€ì˜ ë¬¸ì˜ ë‚´ìš©ìœ¼ë¡œ ë¬¸ì˜ë¥¼ ë“±ë¡í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"
        )
    else:
        current_step = steps[current_idx]
        step_num = current_step["step"]
        total_steps = len(steps)

        response_text = (
            f"**[ë‹¨ê³„ {step_num}/{total_steps}]** {current_step['action']}\n\n"
            f"{current_step['description']}\n\n"
            f"ğŸ“Œ ê¸°ëŒ€ ê²°ê³¼: {current_step['expected_result']}\n\n"
            f"ì´ ë‹¨ê³„ë¥¼ í™•ì¸í•˜ì…¨ë‚˜ìš”? ê²°ê³¼ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”."
        )

    # ì‘ë‹µ ë©”ì‹œì§€ ì¶”ê°€
    state["messages"].append(AIMessage(content=response_text))
    state["status"] = "waiting_user"

    return state
```

### 5. Evaluate Status Node (ìƒíƒœ í‰ê°€)

```python
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate

def evaluate_status_node(state: SupportState) -> SupportState:
    """
    ì‚¬ìš©ì ì‘ë‹µ í‰ê°€
    - ë¬¸ì œê°€ í•´ê²°ë˜ì—ˆëŠ”ì§€ íŒë‹¨
    - ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í• ì§€ ê²°ì •
    """

    llm = ChatOllama(model="gemma2:27b", temperature=0)

    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ì‘ë‹µ ê°€ì ¸ì˜¤ê¸°
    last_user_message = ""
    for msg in reversed(state["messages"]):
        if msg.type == "human":
            last_user_message = msg.content
            break

    # í˜„ì¬ ë‹¨ê³„ ì •ë³´
    current_idx = state["current_step"]
    current_step = state["solution_steps"][current_idx] if current_idx < len(state["solution_steps"]) else None

    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ê³ ê°ì§€ì› ëŒ€í™”ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì‘ë‹µì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ íŒë‹¨í•˜ì„¸ìš”:

1. "resolved": ë¬¸ì œê°€ í•´ê²°ë¨
2. "continue": í˜„ì¬ ë‹¨ê³„ê°€ íš¨ê³¼ ì—†ìŒ, ë‹¤ìŒ ë‹¨ê³„ í•„ìš”
3. "escalate": ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ë¬¸ì˜ ë“±ë¡ ìš”ì²­

íŒë‹¨ ê¸°ì¤€:
- "í•´ê²°ëì–´ìš”", "ëì–´ìš”", "ê°ì‚¬í•©ë‹ˆë‹¤" ë“± â†’ resolved
- "ì•ˆë¼ìš”", "ì—¬ì „íˆ", "ì²´í¬ë˜ì–´ ìˆëŠ”ë°" ë“± â†’ continue
- "ë“±ë¡í•´ì£¼ì„¸ìš”", "ë¬¸ì˜í• ê²Œìš”" ë“± â†’ escalate

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{"decision": "resolved|continue|escalate", "reason": "íŒë‹¨ ì´ìœ "}}"""),
        ("user", """í˜„ì¬ ë‹¨ê³„: {current_step}
ì‚¬ìš©ì ì‘ë‹µ: {user_response}""")
    ])

    response = llm.invoke(
        prompt.format_messages(
            current_step=str(current_step),
            user_response=last_user_message
        )
    )

    try:
        evaluation = json.loads(response.content)
        decision = evaluation["decision"]

        if decision == "resolved":
            state["status"] = "resolved"
            state["messages"].append(
                AIMessage(content="ë¬¸ì œê°€ í•´ê²°ë˜ì–´ ê¸°ì©ë‹ˆë‹¤! ì¶”ê°€ë¡œ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë¬¸ì˜í•´ì£¼ì„¸ìš”.")
            )
        elif decision == "escalate":
            state["status"] = "escalated"
            state["unresolved_reason"] = evaluation["reason"]
        else:  # continue
            # í˜„ì¬ ë‹¨ê³„ë¥¼ ì™„ë£Œë¡œ í‘œì‹œí•˜ê³  ë‹¤ìŒ ë‹¨ê³„ë¡œ
            if current_step:
                current_step["completed"] = True
            state["current_step"] += 1
            state["status"] = "responding"

    except json.JSONDecodeError:
        # ê¸°ë³¸ ë™ì‘: ë‹¤ìŒ ë‹¨ê³„ë¡œ
        state["current_step"] += 1
        state["status"] = "responding"

    return state
```

### 6. Create Ticket Node (í‹°ì¼“ ìƒì„±)

```python
import json
from datetime import datetime
import uuid

def create_ticket_node(state: SupportState) -> SupportState:
    """
    í‹°ì¼“ ìƒì„± ë…¸ë“œ
    - ëŒ€í™” ë‚´ìš© ìš”ì•½
    - Q&A ê²Œì‹œíŒì— ë“±ë¡ (PoC: JSON íŒŒì¼ ì €ì¥)
    """

    llm = ChatOllama(model="gemma2:27b", temperature=0)

    # ëŒ€í™” ë‚´ìš© í¬ë§·íŒ…
    conversation = "\n".join([
        f"{'ì‚¬ìš©ì' if msg.type == 'human' else 'Agent'}: {msg.content}"
        for msg in state["messages"]
    ])

    # ìš”ì•½ ìƒì„±
    summary_prompt = ChatPromptTemplate.from_messages([
        ("system", """ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ Q&A ê²Œì‹œíŒ ì œëª©ê³¼ ë³¸ë¬¸ì„ ì‘ì„±í•˜ì„¸ìš”.

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
  "title": "ê°„ê²°í•œ ì œëª© (20ì ì´ë‚´)",
  "summary": "ë¬¸ì œ ìƒí™© ìš”ì•½ (100ì ì´ë‚´)",
  "attempted_solutions": ["ì‹œë„í•œ í•´ê²°ë°©ë²• 1", "ì‹œë„í•œ í•´ê²°ë°©ë²• 2", ...]
}}"""),
        ("user", "ëŒ€í™” ë‚´ìš©:\n{conversation}")
    ])

    response = llm.invoke(
        summary_prompt.format_messages(conversation=conversation)
    )

    try:
        summary = json.loads(response.content)
    except json.JSONDecodeError:
        summary = {
            "title": "ê³ ê° ë¬¸ì˜",
            "summary": state["current_query"],
            "attempted_solutions": []
        }

    # í‹°ì¼“ ìƒì„±
    ticket = {
        "ticket_id": str(uuid.uuid4())[:8],
        "user_id": state.get("user_id", "anonymous"),
        "session_id": state["session_id"],
        "title": summary["title"],
        "summary": summary["summary"],
        "attempted_solutions": summary["attempted_solutions"],
        "conversation_history": [
            {"role": msg.type, "content": msg.content, "timestamp": datetime.now().isoformat()}
            for msg in state["messages"]
        ],
        "category": state["retrieved_docs"][0]["category"] if state["retrieved_docs"] else "ê¸°íƒ€",
        "status": "open",
        "created_at": datetime.now().isoformat(),
        "answered_at": None,
        "answer": None
    }

    # íŒŒì¼ë¡œ ì €ì¥ (PoC)
    ticket_file = f"data/tickets/ticket_{ticket['ticket_id']}.json"
    with open(ticket_file, "w", encoding="utf-8") as f:
        json.dump(ticket, f, ensure_ascii=False, indent=2)

    state["ticket_id"] = ticket["ticket_id"]
    state["status"] = "ticket_created"

    # ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´
    state["messages"].append(
        AIMessage(content=f"""ì•„ë˜ ë‚´ìš©ìœ¼ë¡œ ë¬¸ì˜ë¥¼ ë“±ë¡í•˜ì˜€ìŠµë‹ˆë‹¤.

**ë¬¸ì˜ ë²ˆí˜¸**: {ticket['ticket_id']}
**ì œëª©**: {ticket['title']}
**ìš”ì•½**: {ticket['summary']}

ë‹µë³€ì´ ë“±ë¡ë˜ë©´ ì´ë©”ì¼ë¡œ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.""")
    )

    return state
```

### 7. Send Notification Node (ì•Œë¦¼ ë°œì†¡)

```python
def send_notification_node(state: SupportState) -> SupportState:
    """
    ì•Œë¦¼ ë°œì†¡ ë…¸ë“œ
    - ì´ë©”ì¼ ì•Œë¦¼ (PoC: ì½˜ì†” ì¶œë ¥)
    - í‘¸ì‹œ ì•Œë¦¼ ì‹œë®¬ë ˆì´ì…˜
    """

    ticket_id = state.get("ticket_id", "N/A")
    user_id = state.get("user_id", "anonymous")

    # ì´ë©”ì¼ ë‚´ìš© ìƒì„±
    email_content = f"""
ì•ˆë…•í•˜ì„¸ìš”,

ë¬¸ì˜ê°€ ì •ìƒì ìœ¼ë¡œ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.

ë¬¸ì˜ë²ˆí˜¸: {ticket_id}
ë“±ë¡ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ë‹´ë‹¹ìê°€ í™•ì¸ í›„ ë‹µë³€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
ë‹µë³€ì´ ë“±ë¡ë˜ë©´ ë‹¤ì‹œ ì•Œë¦¼ì„ ë³´ë‚´ë“œë¦½ë‹ˆë‹¤.

ê°ì‚¬í•©ë‹ˆë‹¤.
    """

    # PoC: ì½˜ì†” ì¶œë ¥
    print("\n" + "="*50)
    print("ğŸ“§ ì´ë©”ì¼ ë°œì†¡ ì‹œë®¬ë ˆì´ì…˜")
    print("="*50)
    print(f"To: user_{user_id}@example.com")
    print(f"Subject: [ê³ ê°ì§€ì›] ë¬¸ì˜ê°€ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤ (#{ticket_id})")
    print(email_content)
    print("="*50 + "\n")

    # ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ”:
    # send_email(
    #     to=user_email,
    #     subject=f"[ê³ ê°ì§€ì›] ë¬¸ì˜ê°€ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤ (#{ticket_id})",
    #     body=email_content
    # )

    return state
```

### 8. Classify Intent Node (ì˜ë„ ë¶„ë¥˜) â­ NEW

```python
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
import json

def classify_intent_node(state: SupportState) -> Dict[str, Any]:
    """
    ì‚¬ìš©ì ì˜ë„ ë¶„ë¥˜ ë…¸ë“œ
    - LLMì„ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ì˜ë„ íŒŒì•…
    - ë¬¸ë§¥ì„ ê³ ë ¤í•œ ë¶„ë¥˜
    """

    # ëŒ€í™” ê³„ì† ì—¬ë¶€ ë¨¼ì € í™•ì¸ (ë¹ ë¥¸ ê²½ë¡œ)
    has_steps = state.get("solution_steps") and len(state.get("solution_steps", [])) > 0
    was_waiting = state.get("status") == "waiting_user"

    if has_steps and was_waiting:
        # ê¸°ì¡´ ëŒ€í™” ê³„ì†
        state["intent"] = "continue_conversation"
        state["status"] = "evaluating"
        return state

    # ìƒˆ ì…ë ¥ì¸ ê²½ìš° LLMìœ¼ë¡œ ë¶„ë¥˜
    last_user_message = ""
    for msg in reversed(state["messages"]):
        if msg.type == "human":
            last_user_message = msg.content
            break

    llm = ChatOllama(model="gemma2:27b", temperature=0)

    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ê³ ê° ë¬¸ì˜ ì˜ë„ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì…ë ¥ì„ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:

1. "small_talk": ì¸ì‚¬, ì¡ë‹´, ê°ì‚¬ ì¸ì‚¬
   ì˜ˆì‹œ: "ì•ˆë…•í•˜ì„¸ìš”", "Hello", "ê°ì‚¬í•©ë‹ˆë‹¤"

2. "technical_support": ê¸°ìˆ  ì§€ì›, ë¬¸ì œ í•´ê²°, ë¬¸ì˜ ìš”ì²­
   ì˜ˆì‹œ: "ë¡œê·¸ì¸ì´ ì•ˆë¼ìš”", "íŒŒì¼ ì—…ë¡œë“œ ì˜¤ë¥˜"

ì¤‘ìš”: ì¸ì‚¬ì™€ ë¬¸ì˜ê°€ í•¨ê»˜ ìˆìœ¼ë©´ "technical_support"ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{"intent": "small_talk/technical_support", "reason": "ë¶„ë¥˜ ì´ìœ ", "confidence": 0.0-1.0}"""),
        ("user", f"ì‚¬ìš©ì ì…ë ¥: {last_user_message}")
    ])

    try:
        response = llm.invoke(prompt.format_messages())
        content = response.content.strip()

        # JSON íŒŒì‹± (ì½”ë“œ ë¸”ë¡ ì œê±°)
        if "```" in content:
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:].strip()

        classification = json.loads(content)
        intent = classification.get("intent", "technical_support")
        confidence = classification.get("confidence", 0.0)

        state["intent"] = intent
        state["intent_confidence"] = confidence

        if intent == "small_talk":
            state["status"] = "small_talking"
        else:  # technical_support
            state["status"] = "searching"

    except (json.JSONDecodeError, Exception) as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ ê¸°ìˆ  ì§€ì›ìœ¼ë¡œ ë¶„ë¥˜
        state["intent"] = "technical_support"
        state["status"] = "searching"

    return state
```

### 9. Handle Small Talk Node (ìŠ¤ëª°í†¡ ì²˜ë¦¬) â­ NEW

```python
def handle_small_talk_node(state: SupportState) -> Dict[str, Any]:
    """
    ìŠ¤ëª°í†¡ ì²˜ë¦¬ ë…¸ë“œ
    - ì¸ì‚¬ë§, ê°ì‚¬ ë“± ê°„ë‹¨í•œ ì‘ë‹µ ì œê³µ
    """

    last_user_message = ""
    for msg in reversed(state["messages"]):
        if msg.type == "human":
            last_user_message = msg.content.lower()
            break

    # ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±
    if any(word in last_user_message for word in ["ì•ˆë…•", "hello", "hi", "ì¢‹ì€"]):
        response = "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜Š"
    elif any(word in last_user_message for word in ["ê°ì‚¬", "ê³ ë§ˆì›Œ", "thanks", "thank"]):
        response = "ì²œë§Œì—ìš”! ë” ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ì£¼ì„¸ìš”. ğŸ˜Š"
    else:
        response = "ë„¤, ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜Š"

    state["messages"].append(AIMessage(content=response))
    state["status"] = "waiting_user"

    return state
```

### 10. Confirm Ticket Node (í‹°ì¼“ í™•ì¸) â­ NEW

```python
def confirm_ticket_node(state: SupportState) -> Dict[str, Any]:
    """
    í‹°ì¼“ ìƒì„± í™•ì¸ ë…¸ë“œ
    - í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš© ìš”ì•½ (LLM ì‚¬ìš©)
    - í‹°ì¼“ ë“±ë¡ ì˜ì‚¬ í™•ì¸
    """

    llm = ChatOllama(model="gemma2:27b", temperature=0)

    # ëŒ€í™” ë‚´ìš© í¬ë§·íŒ…
    conversation_history = []
    for msg in state["messages"]:
        role = "ì‚¬ìš©ì" if msg.type == "human" else "ìƒë‹´ì›"
        conversation_history.append(f"{role}: {msg.content[:100]}..." if len(msg.content) > 100 else f"{role}: {msg.content}")

    conversation_text = "\n".join(conversation_history)

    # ëŒ€í™” ìš”ì•½ ìƒì„± (LLM)
    summary_prompt = ChatPromptTemplate.from_messages([
        ("system", """ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ ê°„ê²°í•œ ì œëª©ì„ ìƒì„±í•˜ì„¸ìš”.

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{
  "title": "ê°„ê²°í•œ ì œëª© (30ì ì´ë‚´, ì£¼ìš” ë¬¸ì œë§Œ)",
  "main_issue": "í•µì‹¬ ë¬¸ì œ ì„¤ëª… (50ì ì´ë‚´)"
}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""),
        ("user", "ëŒ€í™” ë‚´ìš©:\n{conversation}")
    ])

    try:
        full_conversation = "\n".join([
            f"{'ì‚¬ìš©ì' if msg.type == 'human' else 'ìƒë‹´ì›'}: {msg.content}"
            for msg in state["messages"]
        ])

        response = llm.invoke(summary_prompt.format_messages(conversation=full_conversation))
        content = response.content.strip()

        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:].strip()

        summary = json.loads(content)
        title = summary.get("title", "ê³ ê° ë¬¸ì˜")
        main_issue = summary.get("main_issue", state.get("current_query", "ë¬¸ì˜ ë‚´ìš©"))
    except (json.JSONDecodeError, Exception) as e:
        title = "ê³ ê° ë¬¸ì˜"
        main_issue = state.get("current_query", "ë¬¸ì˜ ë‚´ìš©")

    # í™•ì¸ ë©”ì‹œì§€ ìƒì„±
    attempted_steps = state.get("current_step", 0)

    response_text = "ğŸ˜” ë¶ˆí¸ì„ ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤.\n\n"

    if attempted_steps > 0:
        response_text += f"ì§€ê¸ˆê¹Œì§€ {attempted_steps}ë‹¨ê³„ë¥¼ ì‹œë„í•˜ì…¨ì§€ë§Œ ë¬¸ì œê°€ í•´ê²°ë˜ì§€ ì•Šì€ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n"

    response_text += (
        "ë‹´ë‹¹ ë¶€ì„œì˜ í™•ì¸ì´ í•„ìš”í•œ ìƒí™©ì…ë‹ˆë‹¤.\n\n"
        "ğŸ“‹ **ë“±ë¡ë  ë¬¸ì˜ ë‚´ìš©:**\n\n"
        f"**ì œëª©**: {title}\n"
        f"**í•µì‹¬ ë¬¸ì œ**: {main_issue}\n\n"
        "**ëŒ€í™” ë‚´ì—­** (ìµœê·¼ 5ê°œ ë©”ì‹œì§€):\n"
        f"```\n{conversation_text[-5:]}\n```\n\n"
        "ğŸ’¬ **ì´ ë‚´ìš©ìœ¼ë¡œ ë¬¸ì˜ë¥¼ ë“±ë¡í•˜ì‹œê² ìŠµë‹ˆê¹Œ?**\n\n"
        "ë‹µë³€í•´ì£¼ì„¸ìš”:\n"
        "- 'ë„¤' ë˜ëŠ” 'ë“±ë¡í•´ì£¼ì„¸ìš”' â†’ ë¬¸ì˜ ë“±ë¡\n"
        "- 'ì•„ë‹ˆìš”' ë˜ëŠ” 'ì·¨ì†Œ' â†’ ë¬¸ì˜ ë“±ë¡ ì·¨ì†Œ"
    )

    state["messages"].append(AIMessage(content=response_text))
    state["status"] = "confirming_ticket"

    return state
```

### 11. Evaluate Ticket Confirmation Node (í‹°ì¼“ ì‘ë‹µ í‰ê°€) â­ NEW

```python
def evaluate_ticket_confirmation_node(state: SupportState) -> Dict[str, Any]:
    """
    í‹°ì¼“ í™•ì¸ í‰ê°€ ë…¸ë“œ
    - LLMì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ê¸ì •/ë¶€ì • ì˜ì‚¬ë¥¼ ì •í™•íˆ íŒë‹¨
    """

    llm = ChatOllama(model="gemma2:27b", temperature=0)

    last_user_message = ""
    for msg in reversed(state["messages"]):
        if msg.type == "human":
            last_user_message = msg.content
            break

    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì˜ì‚¬ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ìê°€ ë¬¸ì˜ í‹°ì¼“ ë“±ë¡ì„ ì›í•˜ëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.

ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:
1. "yes": í‹°ì¼“ ë“±ë¡ì„ ì›í•¨ (ê¸ì • í‘œí˜„)
   ì˜ˆ: ë„¤, yes, ì‘, ã…‡ã…‡, ì¢‹ì•„, ê·¸ë˜, ë“±ë¡í•´ì¤˜ ë“±
2. "no": í‹°ì¼“ ë“±ë¡ì„ ì›í•˜ì§€ ì•ŠìŒ (ë¶€ì • í‘œí˜„)
   ì˜ˆ: ì•„ë‹ˆ, no, ì•ˆí•´, ì·¨ì†Œ, ì‹«ì–´, ã„´ã„´ ë“±
3. "unclear": ì˜ì‚¬ê°€ ëª…í™•í•˜ì§€ ì•ŠìŒ

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{"decision": "yes/no/unclear", "reason": "íŒë‹¨ ì´ìœ "}"""),
        ("user", f"ì‚¬ìš©ì ì‘ë‹µ: {last_user_message}")
    ])

    try:
        response = llm.invoke(prompt.format_messages())
        content = response.content.strip()

        if "```" in content:
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:].strip()

        evaluation = json.loads(content)
        decision = evaluation.get("decision", "unclear")

        if decision == "yes":
            state["ticket_confirmed"] = True
            state["status"] = "escalated"
        elif decision == "no":
            state["ticket_confirmed"] = False
            state["status"] = "cancelled"
            state["messages"].append(
                AIMessage(content=(
                    "ì•Œê² ìŠµë‹ˆë‹¤. ë¬¸ì˜ ë“±ë¡ì„ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.\n\n"
                    "ë‹¤ë¥¸ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”. ğŸ˜Š"
                ))
            )
        else:  # unclear
            state["ticket_confirmed"] = None
            state["messages"].append(
                AIMessage(content=(
                    "ì£„ì†¡í•©ë‹ˆë‹¤. ëª…í™•í•˜ê²Œ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n\n"
                    "ë¬¸ì˜ë¥¼ ë“±ë¡í•˜ì‹œë ¤ë©´ 'ë„¤' ë˜ëŠ” 'ë“±ë¡í•´ì£¼ì„¸ìš”'ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.\n"
                    "ë“±ë¡ì„ ì›í•˜ì§€ ì•Šìœ¼ì‹œë©´ 'ì•„ë‹ˆìš”' ë˜ëŠ” 'ì·¨ì†Œ'ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”."
                ))
            )

    except (json.JSONDecodeError, Exception) as e:
        state["ticket_confirmed"] = None
        state["messages"].append(
            AIMessage(content=(
                "ì£„ì†¡í•©ë‹ˆë‹¤. ëª…í™•í•˜ê²Œ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n\n"
                "ë¬¸ì˜ë¥¼ ë“±ë¡í•˜ì‹œë ¤ë©´ 'ë„¤' ë˜ëŠ” 'ë“±ë¡í•´ì£¼ì„¸ìš”'ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.\n"
                "ë“±ë¡ì„ ì›í•˜ì§€ ì•Šìœ¼ì‹œë©´ 'ì•„ë‹ˆìš”' ë˜ëŠ” 'ì·¨ì†Œ'ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”."
            ))
        )

    return state
```

### 12. State Reset Utility (ìƒíƒœ ì´ˆê¸°í™”) â­ NEW

```python
def reset_conversation_state(state: SupportState) -> Dict[str, Any]:
    """
    ëŒ€í™” ìƒíƒœ ì´ˆê¸°í™”
    - ìƒˆë¡œìš´ ëŒ€í™”ë¥¼ ì‹œì‘í•  ìˆ˜ ìˆë„ë¡ ìƒíƒœë¥¼ ë¦¬ì…‹
    - session_id, user_id, messagesëŠ” ìœ ì§€ (ëŒ€í™” ì—°ì†ì„±)
    - ë¬¸ì œ í•´ê²° ê´€ë ¨ ìƒíƒœëŠ” ëª¨ë‘ ì´ˆê¸°í™”
    """

    state["solution_steps"] = []
    state["current_step"] = 0
    state["retrieved_docs"] = []
    state["relevance_score"] = 0.0
    state["unresolved_reason"] = None
    state["ticket_id"] = None
    state["is_continuing"] = False
    state["attempts"] = 0
    state["intent"] = None
    state["intent_confidence"] = None
    state["ticket_confirmed"] = None
    state["current_query"] = ""

    return state
```

**ì°¸ê³ **: ì´ í•¨ìˆ˜ëŠ” `evaluate_status_node`ì™€ `create_ticket_node`ì—ì„œ ë¬¸ì œ í•´ê²° ë˜ëŠ” í‹°ì¼“ ìƒì„± ì™„ë£Œ í›„ í˜¸ì¶œë©ë‹ˆë‹¤.

---

## Human-in-the-Loop êµ¬í˜„

### Interrupt ì‚¬ìš©

LangGraphëŠ” íŠ¹ì • ë…¸ë“œ ì „í›„ì— `interrupt` ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

```python
from langgraph.graph import StateGraph

# ì»´íŒŒì¼ ì‹œ interrupt ì„¤ì •
app = workflow.compile(
    checkpointer=memory,
    interrupt_before=["evaluate_status"]  # í‰ê°€ ì „ì— ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°
)
```

### ì‹¤í–‰ íë¦„

```python
from langchain_core.messages import HumanMessage

# ì„¤ì •
config = {
    "configurable": {
        "thread_id": "user_session_123"  # ëŒ€í™” ìŠ¤ë ˆë“œ ID
    }
}

# 1. ì²« ì§ˆì˜
initial_input = {
    "messages": [HumanMessage(content="ë©”ì‹ ì €ì—ì„œ ì‹ ì°© ë©”ì‹œì§€ ì•Œë¦¼ì´ ì•ˆë– ìš”")],
    "user_id": "user_001"
}

# ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (interruptê¹Œì§€)
for event in app.stream(initial_input, config):
    print(event)

# 2. ì‚¬ìš©ì ì‘ë‹µ í›„ ì¬ê°œ
user_response = {
    "messages": [HumanMessage(content="ì²´í¬ë˜ì–´ ìˆëŠ”ë°ìš”")]
}

# ì´ì „ ìƒíƒœì—ì„œ ê³„ì†
for event in app.stream(user_response, config):
    print(event)

# 3. ê³„ì† ì§„í–‰
user_response_2 = {
    "messages": [HumanMessage(content="ê·¸ê²ƒë„ ì¼¬ìœ¼ë¡œ ë˜ì–´ ìˆì–´ìš”")]
}

for event in app.stream(user_response_2, config):
    print(event)

# 4. í‹°ì¼“ ë“±ë¡ ìš”ì²­
user_response_3 = {
    "messages": [HumanMessage(content="ë„¤ ë“±ë¡í•´ì£¼ì„¸ìš”")]
}

for event in app.stream(user_response_3, config):
    print(event)
```

### ìƒíƒœ í™•ì¸ ë° ë³µì›

```python
# í˜„ì¬ ìƒíƒœ í™•ì¸
current_state = app.get_state(config)
print(f"í˜„ì¬ ìƒíƒœ: {current_state.values['status']}")
print(f"í˜„ì¬ ë‹¨ê³„: {current_state.values['current_step']}")

# íŠ¹ì • ì‹œì ìœ¼ë¡œ ë˜ëŒë¦¬ê¸° (Time Travel)
history = app.get_state_history(config)
for state in history:
    print(f"ì‹œì : {state.config['configurable']['checkpoint_id']}")
    print(f"ìƒíƒœ: {state.values['status']}")
```

---

## ê¸°ìˆ  ìŠ¤íƒ

### í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬

```python
# requirements.txt

# LangChain ë° LangGraph
langchain==0.1.0
langchain-core==0.1.0
langchain-community==0.0.13
langgraph==0.0.20

# Ollama (ë¡œì»¬ LLM)
langchain-ollama==0.1.0
ollama==0.1.6

# ë²¡í„° ìŠ¤í† ì–´
chromadb==0.4.22

# Streamlit WebUI
streamlit==1.28.0
streamlit-chat==0.1.1

# ë°ì´í„° ì²˜ë¦¬
pandas==2.1.4
numpy==1.26.2

# ìœ í‹¸ë¦¬í‹°
python-dotenv==1.0.0
pydantic==2.5.3

# ê°œë°œ ë„êµ¬
jupyter==1.0.0
pytest==7.4.3
```

### Ollama ëª¨ë¸ ì„¤ì¹˜

```bash
# Ollama ì„¤ì¹˜ (https://ollama.ai)
# macOS/Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# https://ollama.ai/download ì—ì„œ ë‹¤ìš´ë¡œë“œ

# í•„ìš”í•œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull gemma2:27b          # LLM ëª¨ë¸
ollama pull bge-m3-korean       # í•œê¸€ ì„ë² ë”© ëª¨ë¸ (1.2GB)

# ëª¨ë¸ í™•ì¸
ollama list
```

### í™˜ê²½ ë³€ìˆ˜ (.env)

```bash
# Ollama ì„¤ì •
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_LLM_MODEL=gemma2:27b
OLLAMA_EMBEDDING_MODEL=bge-m3-korean

# ë°ì´í„° ê²½ë¡œ
DATA_DIR=./data
VECTORSTORE_PATH=./data/vectorstore
TICKETS_PATH=./data/tickets

# Streamlit ì„¤ì •
STREAMLIT_SERVER_PORT=8501
STREAMLIT_SERVER_ADDRESS=localhost

# ë¡œê¹…
LOG_LEVEL=INFO
```

---

## ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ë° ì²­í‚¹ ì „ëµ

### ì²­í‚¹ ì „ëµì˜ ì¤‘ìš”ì„±

ì¼ë°˜ RAGì—ì„œ **ì²­í‚¹(Chunking) ì „ëµ**ì€ ê²€ìƒ‰ í’ˆì§ˆì„ ê²°ì •í•˜ëŠ” ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤. íŠ¹íˆ FAQì²˜ëŸ¼ êµ¬ì¡°í™”ëœ ë°ì´í„°ì—ì„œ ì˜ëª»ëœ ì²­í‚¹ì€ í•´ê²° ë°©ë²•ì´ ì¤‘ê°„ì— ì˜ë ¤ ë¶ˆì™„ì „í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë¬¸ì œë¥¼ ì•¼ê¸°í•©ë‹ˆë‹¤.

### ë¬¸ì œ ìƒí™©: ì¼ë°˜ ì²­í‚¹ì˜ í•œê³„

```python
# âŒ ë‚˜ìœ ì˜ˆ: ê³ ì • ê¸¸ì´ ì²­í‚¹ (500ì ë‹¨ìœ„)
from langchain.text_splitter import CharacterTextSplitter

splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_text(faq_content)

# ê²°ê³¼:
# Chunk 1: "ì œëª© + ì¦ìƒ + ì›ì¸ + ë°©ë²•1ì˜ ì¼ë¶€ë§Œ"
# Chunk 2: "ë°©ë²•1 ë‚˜ë¨¸ì§€ + ë°©ë²•2 ì¼ë¶€"  â† ë¶ˆì™„ì „í•œ ì •ë³´!
# Chunk 3: "ë°©ë²•2 ë‚˜ë¨¸ì§€ + ë°©ë²•3"
```

**ë¬¸ì œì **:
- í•´ê²° ë°©ë²•(Method)ì´ ì—¬ëŸ¬ ì²­í¬ì— ë¶„ì‚°
- ë²¡í„° ê²€ìƒ‰ ì‹œ ë¶ˆì™„ì „í•œ ë‹¨ê³„ë§Œ ë°˜í™˜ ê°€ëŠ¥
- ì‚¬ìš©ìì—ê²Œ ì¤‘ê°„ì´ ì˜ë¦° ì§€ì¹¨ ì œê³µ â†’ ë¬¸ì œ í•´ê²° ì‹¤íŒ¨

### ê¶Œì¥ ì²­í‚¹ ì „ëµ

#### ì „ëµ 1: ë¬¸ì„œ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ (ê¶Œì¥ â­)

FAQ ë¬¸ì„œëŠ” ì´ë¯¸ ì˜ë¯¸ì ìœ¼ë¡œ ì™„ê²°ëœ ë‹¨ìœ„ì´ë©°, í¬ê¸°ë„ ì ë‹¹í•©ë‹ˆë‹¤.

```python
# âœ… ì¢‹ì€ ì˜ˆ: ë¬¸ì„œ ë‹¨ìœ„ ì²­í‚¹
from langchain_core.documents import Document

def build_vectorstore_whole_document():
    """ê° FAQ ë¬¸ì„œ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ì²˜ë¦¬"""
    documents = []

    for faq in faq_data:
        # ì „ì²´ FAQ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ
        content = f"""ì œëª©: {faq['title']}
ì¹´í…Œê³ ë¦¬: {faq['category']}

ì¦ìƒ:
{faq['content']['symptom']}

ì›ì¸:
{faq['content']['cause']}

í•´ê²° ë°©ë²•:
"""
        # ê° í•´ê²° ë°©ë²•ì„ ì™„ì „í•˜ê²Œ í¬í•¨
        for solution in faq['content']['solutions']:
            content += f"\n[ë°©ë²• {solution['method']}] {solution['title']}\n"
            for i, step in enumerate(solution['steps'], 1):
                content += f"  {i}. {step}\n"
            content += f"  â–¶ ê¸°ëŒ€ ê²°ê³¼: {solution['expected_result']}\n"

        # ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ Document ìƒì„±
        doc = Document(
            page_content=content,
            metadata={
                "id": faq["id"],
                "category": faq["category"],
                "title": faq["title"],
                "tags": faq["tags"],
                "source": faq["source"],
                "helpful_count": faq["helpful_count"]  # í’ˆì§ˆ ì§€í‘œ
            }
        )
        documents.append(doc)

    return documents

# ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•
embeddings = OllamaEmbeddings(model="bge-m3-korean")
vectorstore = Chroma.from_documents(
    documents=build_vectorstore_whole_document(),
    embedding=embeddings,
    persist_directory="data/vectorstore"
)
```

**ì¥ì **:
- âœ… í•´ê²° ë°©ë²•ì´ ì ˆëŒ€ ì˜ë¦¬ì§€ ì•ŠìŒ
- âœ… FAQ ë¬¸ì„œ í¬ê¸°ê°€ ì ë‹¹ (í‰ê·  1000-2000ì)
- âœ… êµ¬í˜„ì´ ë‹¨ìˆœí•˜ê³  ì•ˆì •ì 
- âœ… ì „ì²´ ë§¥ë½(ì¦ìƒ/ì›ì¸/í•´ê²°ì±…) ìœ ì§€

**ë‹¨ì **:
- ë¬¸ì„œê°€ ë§¤ìš° ê¸´ ê²½ìš° ì„ë² ë”© í’ˆì§ˆ ì €í•˜ ê°€ëŠ¥ (í•˜ì§€ë§Œ FAQëŠ” ëŒ€ë¶€ë¶„ ì§§ìŒ)

#### ì „ëµ 2: Parent-Child Document ì „ëµ (ê³ ê¸‰)

ë¬¸ì„œê°€ ê¸´ ê²½ìš°, ê²€ìƒ‰ì€ ì‘ì€ ë‹¨ìœ„ë¡œ í•˜ë˜ ì „ì²´ ë¬¸ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

```python
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore

def build_parent_child_vectorstore():
    """Parent: ì „ì²´ FAQ, Child: ê° í•´ê²° ë°©ë²•"""

    parent_docs = []
    child_docs = []

    for faq in faq_data:
        # Parent: ì „ì²´ FAQ ë¬¸ì„œ
        parent_content = f"{faq['title']}\n{faq['content']['symptom']}\n{faq['content']['cause']}"
        parent_doc = Document(
            page_content=parent_content,
            metadata={"id": faq["id"], "type": "parent"}
        )
        parent_docs.append(parent_doc)

        # Child: ê° í•´ê²° ë°©ë²• (ê²€ìƒ‰ ëŒ€ìƒ)
        for solution in faq['content']['solutions']:
            child_content = f"[ë°©ë²• {solution['method']}] {solution['title']}\n"
            child_content += "\n".join([f"{i}. {step}" for i, step in enumerate(solution['steps'], 1)])
            child_content += f"\nê¸°ëŒ€ ê²°ê³¼: {solution['expected_result']}"

            child_doc = Document(
                page_content=child_content,
                metadata={
                    "parent_id": faq["id"],
                    "method": solution["method"],
                    "type": "child"
                }
            )
            child_docs.append(child_doc)

    # ParentDocumentRetriever ì‚¬ìš©
    embeddings = OllamaEmbeddings(model="bge-m3-korean")
    vectorstore = Chroma.from_documents(child_docs, embeddings)
    docstore = InMemoryStore()

    retriever = ParentDocumentRetriever(
        vectorstore=vectorstore,
        docstore=docstore,
        child_splitter=None,  # ì´ë¯¸ ìˆ˜ë™ìœ¼ë¡œ ë¶„í• 
        parent_splitter=None
    )

    return retriever
```

**ì¥ì **:
- âœ… ì •ë°€í•œ ê²€ìƒ‰ (ê° í•´ê²° ë°©ë²• ë‹¨ìœ„)
- âœ… ì™„ì „í•œ ë¬¸ì„œ ë°˜í™˜ (Parent)
- âœ… ê¸´ ë¬¸ì„œì—ë„ íš¨ê³¼ì 

**ë‹¨ì **:
- êµ¬í˜„ ë³µì¡ë„ ì¦ê°€
- ì¶”ê°€ ì €ì¥ì†Œ(docstore) í•„ìš”

#### ì „ëµ 3: ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ (Semantic Chunking)

êµ¬ì¡°í™”ëœ í•„ë“œ(ì¦ìƒ, ì›ì¸, ê° ë°©ë²•)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì²­í‚¹í•©ë‹ˆë‹¤.

```python
def build_semantic_chunks():
    """ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í‚¹ - ê° í•´ê²° ë°©ë²•ì„ ë…ë¦½ì ì¸ ì²­í¬ë¡œ"""
    documents = []

    for faq in faq_data:
        # ê° í•´ê²° ë°©ë²•ì„ ë³„ë„ ì²­í¬ë¡œ (í•˜ì§€ë§Œ ë§¥ë½ ì •ë³´ í¬í•¨)
        base_context = f"[FAQ-{faq['id']}] {faq['title']}\nì¦ìƒ: {faq['content']['symptom']}\nì›ì¸: {faq['content']['cause']}\n\n"

        for solution in faq['content']['solutions']:
            # ë§¥ë½(ì¦ìƒ/ì›ì¸) + ì™„ì „í•œ í•´ê²° ë°©ë²•
            content = base_context
            content += f"í•´ê²° ë°©ë²• {solution['method']}: {solution['title']}\n"
            content += "\n".join([f"{i}. {step}" for i, step in enumerate(solution['steps'], 1)])
            content += f"\nê¸°ëŒ€ ê²°ê³¼: {solution['expected_result']}"

            doc = Document(
                page_content=content,
                metadata={
                    "faq_id": faq["id"],
                    "category": faq["category"],
                    "method_number": solution["method"],
                    "chunk_type": "solution"
                }
            )
            documents.append(doc)

    return documents
```

**ì¥ì **:
- âœ… ê° í•´ê²° ë°©ë²•ì´ ì™„ì „í•˜ê²Œ ìœ ì§€
- âœ… ë§¥ë½ ì •ë³´(ì¦ìƒ/ì›ì¸) í¬í•¨
- âœ… ê²€ìƒ‰ ì •ë°€ë„ í–¥ìƒ

**ë‹¨ì **:
- ì¤‘ë³µ ì •ë³´ (ê° ì²­í¬ì— ì¦ìƒ/ì›ì¸ ë°˜ë³µ)
- ë²¡í„° DB í¬ê¸° ì¦ê°€

### PoC ê¶Œì¥ êµ¬í˜„

**Phase 1 (PoC)**: ì „ëµ 1 - ë¬¸ì„œ ì „ì²´ ì²­í‚¹ ì‚¬ìš©
- FAQ ë¬¸ì„œ í¬ê¸°ê°€ ì ë‹¹ (í‰ê·  1000-2000ì)
- êµ¬í˜„ ë‹¨ìˆœ, ì•ˆì •ì 
- í•´ê²° ë°©ë²• ì˜ë¦¼ ìœ„í—˜ ì œë¡œ

**Phase 2 (í”„ë¡œë•ì…˜)**: ì „ëµ 2 ë˜ëŠ” 3 ê³ ë ¤
- ì‹¤ì œ ë°ì´í„° ë¶„ì„ í›„ ê²°ì •
- A/B í…ŒìŠ¤íŠ¸ë¡œ ê²€ìƒ‰ í’ˆì§ˆ ë¹„êµ

### ë©”íƒ€ë°ì´í„° í•„í„°ë§ í™œìš©

ì²­í‚¹ê³¼ í•¨ê»˜ ë©”íƒ€ë°ì´í„° í•„í„°ë§ì„ í™œìš©í•˜ë©´ ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë”ìš± ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ í•„í„°ë§
results = vectorstore.similarity_search(
    query="ë©”ì‹ ì € ì•Œë¦¼ì´ ì•ˆë– ìš”",
    k=3,
    filter={"category": "ë©”ì‹ ì €"}  # ì¹´í…Œê³ ë¦¬ í•„í„°
)

# í’ˆì§ˆ ê¸°ë°˜ í•„í„°ë§ (helpful_count ë†’ì€ ê²ƒ ìš°ì„ )
results = vectorstore.similarity_search(
    query="ë¡œê·¸ì¸ ì˜¤ë¥˜",
    k=5,
    filter={"helpful_count": {"$gte": 100}}  # ë„ì›€ë¨ 100ê°œ ì´ìƒ
)
```

### ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸ ì˜ˆì‹œ

```python
# scripts/build_vectorstore.py

import json
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

def load_faq_data(file_path: str):
    """FAQ JSON íŒŒì¼ ë¡œë“œ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def create_documents_from_faq(faq_data: list) -> list[Document]:
    """FAQ ë°ì´í„°ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜ (ì „ì²´ ë¬¸ì„œ ì²­í‚¹)"""
    documents = []

    for faq in faq_data:
        # ì „ì²´ FAQ ë‚´ìš© êµ¬ì„±
        content = f"""ì œëª©: {faq['title']}
ì¹´í…Œê³ ë¦¬: {faq['category']}

ì¦ìƒ:
{faq['content']['symptom']}

ì›ì¸:
{faq['content']['cause']}

í•´ê²° ë°©ë²•:
"""
        for solution in faq['content']['solutions']:
            content += f"\n[ë°©ë²• {solution['method']}] {solution['title']}\n"
            for i, step in enumerate(solution['steps'], 1):
                content += f"  {i}. {step}\n"
            content += f"  â–¶ ê¸°ëŒ€ ê²°ê³¼: {solution['expected_result']}\n"

        doc = Document(
            page_content=content,
            metadata={
                "id": faq["id"],
                "category": faq["category"],
                "title": faq["title"],
                "tags": faq["tags"],
                "source": faq["source"],
                "helpful_count": faq["helpful_count"],
                "created_at": faq["created_at"]
            }
        )
        documents.append(doc)

    return documents

def main():
    print("ğŸ“š FAQ ë°ì´í„° ë¡œë“œ ì¤‘...")
    faq_data = load_faq_data("data/faq_1000.json")
    print(f"âœ… {len(faq_data)}ê°œ FAQ ë¡œë“œ ì™„ë£Œ")

    print("\nğŸ“„ Document ê°ì²´ ìƒì„± ì¤‘...")
    documents = create_documents_from_faq(faq_data)
    print(f"âœ… {len(documents)}ê°œ Document ìƒì„± ì™„ë£Œ")

    print("\nğŸ”„ Ollama ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    embeddings = OllamaEmbeddings(model="bge-m3-korean")
    print("âœ… BGE-M3-Korean ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    print("\nğŸ—„ï¸  Chroma ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì¤‘...")
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        persist_directory="data/vectorstore",
        collection_name="faq_collection"
    )
    print("âœ… ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ")

    # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
    print("\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...")
    test_query = "ë©”ì‹ ì €ì—ì„œ ì•Œë¦¼ì´ ì•ˆë– ìš”"
    results = vectorstore.similarity_search(test_query, k=3)

    print(f"\nì¿¼ë¦¬: {test_query}")
    print(f"ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
    for i, doc in enumerate(results, 1):
        print(f"\n[{i}] {doc.metadata['title']}")
        print(f"    ì¹´í…Œê³ ë¦¬: {doc.metadata['category']}")
        print(f"    ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {doc.page_content[:100]}...")

    print("\nâœ… ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ!")

if __name__ == "__main__":
    main()
```

### ì²­í‚¹ í’ˆì§ˆ ê²€ì¦

```python
# scripts/validate_chunking.py

def validate_chunking_strategy():
    """ì²­í‚¹ ì „ëµ ê²€ì¦: í•´ê²° ë°©ë²•ì´ ì™„ì „í•œì§€ í™•ì¸"""
    vectorstore = Chroma(
        persist_directory="data/vectorstore",
        embedding_function=OllamaEmbeddings(model="bge-m3-korean")
    )

    test_cases = [
        "ë©”ì‹ ì € ì•Œë¦¼ ì„¤ì •",
        "ë¡œê·¸ì¸ ë¹„ë°€ë²ˆí˜¸ ì˜¤ë¥˜",
        "íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨"
    ]

    for query in test_cases:
        print(f"\nì¿¼ë¦¬: {query}")
        results = vectorstore.similarity_search(query, k=1)

        doc = results[0]
        content = doc.page_content

        # í•´ê²° ë°©ë²•ì´ ì™„ì „í•œì§€ í™•ì¸
        method_count = content.count("[ë°©ë²•")
        complete_methods = content.count("ê¸°ëŒ€ ê²°ê³¼:")

        print(f"  ë°œê²¬ëœ ë°©ë²• ìˆ˜: {method_count}")
        print(f"  ì™„ì „í•œ ë°©ë²• ìˆ˜: {complete_methods}")

        if method_count != complete_methods:
            print(f"  âš ï¸  ê²½ê³ : ë¶ˆì™„ì „í•œ í•´ê²° ë°©ë²• ë°œê²¬!")
        else:
            print(f"  âœ… ëª¨ë“  í•´ê²° ë°©ë²•ì´ ì™„ì „í•¨")

if __name__ == "__main__":
    validate_chunking_strategy()
```

---

## ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
customer-support-chatbot-poc/
â”œâ”€â”€ README.md                       # í”„ë¡œì íŠ¸ ê°œìš”
â”œâ”€â”€ requirements.txt                # Python ì˜ì¡´ì„±
â”œâ”€â”€ .env                            # í™˜ê²½ ë³€ìˆ˜
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ docs/                           # ë¬¸ì„œ
â”‚   â”œâ”€â”€ customer-support-chatbot-langgraph-design.md
â”‚   â”œâ”€â”€ microsoft-agent-framework-detailed.md
â”‚   â””â”€â”€ user-scenario-workflow.md  # ì‚¬ìš©ì ì‹œë‚˜ë¦¬ì˜¤
â”‚
â”œâ”€â”€ data/                           # ë°ì´í„° ë””ë ‰í† ë¦¬
â”‚   â”œâ”€â”€ faq_1000.json              # 1000ê°œ FAQ ë°ì´í„°
â”‚   â”œâ”€â”€ vectorstore/               # Chroma ë²¡í„° ìŠ¤í† ì–´
â”‚   â”‚   â””â”€â”€ chroma.sqlite3
â”‚   â””â”€â”€ tickets/                   # ìƒì„±ëœ í‹°ì¼“ë“¤
â”‚       â””â”€â”€ ticket_*.json
â”‚
â”œâ”€â”€ src/                           # ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                    # ë°ì´í„° ëª¨ë¸
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ state.py              # State ì •ì˜
â”‚   â”‚   â”œâ”€â”€ faq.py                # FAQ ëª¨ë¸ (ì¦ìƒ/ì›ì¸/ì„ì‹œì¡°ì¹˜)
â”‚   â”‚   â””â”€â”€ ticket.py             # Ticket ëª¨ë¸
â”‚   â”‚
â”‚   â”œâ”€â”€ nodes/                     # LangGraph ë…¸ë“œ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ initialize.py         # ì´ˆê¸°í™” ë…¸ë“œ
â”‚   â”‚   â”œâ”€â”€ search_knowledge.py   # ê²€ìƒ‰ ë…¸ë“œ
â”‚   â”‚   â”œâ”€â”€ plan_response.py      # ê³„íš ë…¸ë“œ
â”‚   â”‚   â”œâ”€â”€ respond_step.py       # ì‘ë‹µ ë…¸ë“œ
â”‚   â”‚   â”œâ”€â”€ evaluate_status.py    # í‰ê°€ ë…¸ë“œ
â”‚   â”‚   â”œâ”€â”€ create_ticket.py      # í‹°ì¼“ ìƒì„±
â”‚   â”‚   â””â”€â”€ send_notification.py  # ì•Œë¦¼ ë°œì†¡
â”‚   â”‚
â”‚   â”œâ”€â”€ graph/                     # ê·¸ë˜í”„ êµ¬ì„±
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ workflow.py           # ì›Œí¬í”Œë¡œìš° ì •ì˜
â”‚   â”‚   â””â”€â”€ routing.py            # ë¼ìš°íŒ… ë¡œì§
â”‚   â”‚
â”‚   â”œâ”€â”€ services/                  # ì„œë¹„ìŠ¤ ë ˆì´ì–´
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ vectorstore.py        # Chroma ë²¡í„° ìŠ¤í† ì–´ ê´€ë¦¬
â”‚   â”‚   â”œâ”€â”€ ollama_service.py     # Ollama LLM í˜¸ì¶œ
â”‚   â”‚   â”œâ”€â”€ ticket_service.py     # í‹°ì¼“ ê´€ë¦¬
â”‚   â”‚   â””â”€â”€ status_service.py     # ì§„í–‰ ìƒíƒœ ê´€ë¦¬
â”‚   â”‚
â”‚   â”œâ”€â”€ ui/                        # Streamlit WebUI
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ app.py                # ë©”ì¸ Streamlit ì•±
â”‚   â”‚   â”œâ”€â”€ components/           # UI ì»´í¬ë„ŒíŠ¸
â”‚   â”‚   â”‚   â”œâ”€â”€ chat_interface.py # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
â”‚   â”‚   â”‚   â”œâ”€â”€ status_display.py # ìƒíƒœ í‘œì‹œ
â”‚   â”‚   â”‚   â””â”€â”€ ticket_view.py    # í‹°ì¼“ ë·°
â”‚   â”‚   â””â”€â”€ styles.py             # CSS ìŠ¤íƒ€ì¼
â”‚   â”‚
â”‚   â””â”€â”€ utils/                     # ìœ í‹¸ë¦¬í‹°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py             # ë¡œê¹… ì„¤ì •
â”‚       â””â”€â”€ config.py             # ì„¤ì • ê´€ë¦¬
â”‚
â”œâ”€â”€ scripts/                       # ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ prepare_faq_data.py       # FAQ 1000ê°œ ë°ì´í„° ì¤€ë¹„
â”‚   â”œâ”€â”€ build_vectorstore.py      # Chroma ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•
â”‚   â””â”€â”€ test_ollama.py            # Ollama ì—°ê²° í…ŒìŠ¤íŠ¸
â”‚
â”œâ”€â”€ notebooks/                     # Jupyter ë…¸íŠ¸ë¶
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_vectorstore_test.ipynb
â”‚   â””â”€â”€ 03_workflow_test.ipynb
â”‚
â”œâ”€â”€ tests/                         # í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_nodes.py
â”‚   â”œâ”€â”€ test_workflow.py
â”‚   â””â”€â”€ test_integration.py
â”‚
â”œâ”€â”€ checkpoints.db                 # SQLite ì²´í¬í¬ì¸íŠ¸
â””â”€â”€ main.py                        # CLI ì‹¤í–‰ íŒŒì¼
```

---

## êµ¬í˜„ ë‹¨ê³„

### Phase 1: í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ì¤€ë¹„ (1ì¼)

**Task 1.1: í”„ë¡œì íŠ¸ ì´ˆê¸°í™”**
```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p data/{vectorstore,tickets}
mkdir -p src/{models,nodes,graph,services,utils}
```

**Task 1.2: ìƒ˜í”Œ ë°ì´í„° ìƒì„±**
- FAQ ë°ì´í„° 20-30ê°œ ì‘ì„± (JSON)
- Q&A ê²Œì‹œíŒ ë°ì´í„° 10-15ê°œ ì‘ì„±
- ì¹´í…Œê³ ë¦¬: ë©”ì‹ ì €, ë¡œê·¸ì¸, ì•Œë¦¼, ë„¤íŠ¸ì›Œí¬ ë“±

**Task 1.3: ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•**
```bash
python scripts/build_vectorstore.py
```

### Phase 2: ëª¨ë¸ ë° ë…¸ë“œ êµ¬í˜„ (2-3ì¼)

**Task 2.1: ë°ì´í„° ëª¨ë¸ ì •ì˜**
- `src/models/state.py` - SupportState
- `src/models/faq.py` - FAQDocument
- `src/models/ticket.py` - Ticket

**Task 2.2: ë…¸ë“œ êµ¬í˜„**
- ê° ë…¸ë“œë³„ êµ¬í˜„ (7ê°œ ë…¸ë“œ)
- ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±

**Task 2.3: ì„œë¹„ìŠ¤ ë ˆì´ì–´ êµ¬í˜„**
- VectorStore ê´€ë¦¬
- LLM í˜¸ì¶œ ë˜í¼
- í‹°ì¼“ CRUD

### Phase 3: ì›Œí¬í”Œë¡œìš° êµ¬ì„± (2ì¼)

**Task 3.1: StateGraph êµ¬ì„±**
- ë…¸ë“œ ì—°ê²°
- ì—£ì§€ ì •ì˜
- ë¼ìš°íŒ… ë¡œì§

**Task 3.2: Checkpointer ì„¤ì •**
- SQLite ê¸°ë°˜ ìƒíƒœ ì €ì¥
- Thread ê´€ë¦¬

### Phase 4: CLI ì¸í„°í˜ì´ìŠ¤ (1ì¼)

**Task 4.1: main.py êµ¬í˜„**
- ëŒ€í™”í˜• CLI
- ìƒíƒœ ì¶œë ¥
- ë””ë²„ê¹… ëª¨ë“œ

### Phase 5: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ (1-2ì¼)

**Task 5.1: í†µí•© í…ŒìŠ¤íŠ¸**
- ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
- ì—£ì§€ ì¼€ì´ìŠ¤ ê²€ì¦

**Task 5.2: ì„±ëŠ¥ ì¸¡ì •**
- ê²€ìƒ‰ ì •í™•ë„
- ì‘ë‹µ ì‹œê°„
- ì‚¬ìš©ì ê²½í—˜

### ì´ ì˜ˆìƒ ê¸°ê°„: 7-9ì¼

---

## ë‹¤ìŒ ë‹¨ê³„

ì´ ì„¤ê³„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì‘ì—…ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. âœ… **ìƒ˜í”Œ ë°ì´í„° ìƒì„±**: FAQ/Q&A JSON íŒŒì¼ ì‘ì„±
2. âœ… **í”„ë¡œì íŠ¸ ìŠ¤ìºí´ë”©**: ë””ë ‰í† ë¦¬ êµ¬ì¡° ë° ê¸°ë³¸ íŒŒì¼ ìƒì„±
3. âœ… **í•µì‹¬ ë…¸ë“œ êµ¬í˜„**: 7ê°œ ë…¸ë“œ ì½”ë“œ ì‘ì„±
4. âœ… **ì›Œí¬í”Œë¡œìš° í†µí•©**: LangGraph êµ¬ì„±
5. âœ… **CLI ì‹¤í–‰**: ì‹¤ì œ ëŒ€í™” í…ŒìŠ¤íŠ¸

ì–´ë–¤ ë¶€ë¶„ë¶€í„° êµ¬í˜„ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ?
