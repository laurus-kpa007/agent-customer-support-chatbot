"""Handle Small Talk Node - ìŠ¤ëª°í†¡ ì²˜ë¦¬

ì¸ì‚¬ë§ ë“± ì¼ë°˜ì ì¸ ëŒ€í™”ì— ëŒ€ì‘í•©ë‹ˆë‹¤.
"""

import sys
from pathlib import Path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from typing import Dict, Any
import os
from langchain_core.messages import AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama

from src.models.state import SupportState


def handle_small_talk_node(state: SupportState) -> Dict[str, Any]:
    """
    ìŠ¤ëª°í†¡ ì²˜ë¦¬ ë…¸ë“œ
    - ì¸ì‚¬ë§ì— ì ì ˆíˆ ì‘ë‹µ
    - ë„ì›€ì´ í•„ìš”í•œì§€ ë¬¼ì–´ë´„

    Args:
        state: í˜„ì¬ ìƒíƒœ

    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (messagesì— ì‘ë‹µ ì¶”ê°€)
    """

    # LLM ì´ˆê¸°í™”
    llm_model = os.getenv("OLLAMA_LLM_MODEL", "gemma2:27b")
    llm = ChatOllama(
        model=llm_model,
        base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
        temperature=0.7  # ì•½ê°„ì˜ ì°½ì˜ì„± í—ˆìš©
    )

    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
    last_user_message = ""
    for msg in reversed(state["messages"]):
        if msg.type == "human":
            last_user_message = msg.content
            break

    # ìŠ¤ëª°í†¡ ì‘ë‹µ í”„ë¡¬í”„íŠ¸
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ê³ ê° ì§€ì› AI ìƒë‹´ì›ì…ë‹ˆë‹¤.
        
        ì‚¬ìš©ìì˜ ì¸ì‚¬ë‚˜ ì¡ë‹´ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ½ê³  ì¸ê°„ì ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
        ì‘ë‹µ í›„ì—ëŠ” ë¶€ë“œëŸ½ê²Œ ë„ì›€ì´ í•„ìš”í•œ ë¶€ë¶„ì´ ìˆëŠ”ì§€ ë¬¼ì–´ë³´ì„¸ìš”.
        
        ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œë“¤ì„ ë„ì™€ì¤„ ìˆ˜ ìˆìŒì„ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•´ë„ ì¢‹ìŠµë‹ˆë‹¤(ë§¤ë²ˆ ì–¸ê¸‰í•  í•„ìš”ëŠ” ì—†ìŒ):
        - ë¡œê·¸ì¸/ë¹„ë°€ë²ˆí˜¸ ë¬¸ì œ
        - ë©”ì‹ ì € ê¸°ëŠ¥ ì˜¤ë¥˜
        - íŒŒì¼ ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œ ë¬¸ì œ
        - ê³„ì • ê´€ë ¨ ë¬¸ì˜
        
        ì–´ì¡°:
        - ê³µê°í•˜ê³  ì¹œê·¼í•˜ê²Œ
        - ì´ëª¨ì§€(ğŸ‘‹, ğŸ˜Š ë“±)ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ë”±ë”±í•˜ì§€ ì•Šê²Œ
        - ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ ê°„ê²°í•˜ê²Œ
        """),
        ("user", f"{last_user_message}")
    ])

    # LLM í˜¸ì¶œ ë° ì‘ë‹µ ìƒì„±
    chain = prompt | llm
    response = chain.invoke({})
    response_text = response.content

    # ì‘ë‹µ ë©”ì‹œì§€ ì¶”ê°€
    state["messages"].append(AIMessage(content=response_text))
    state["status"] = "waiting_user"

    # ìŠ¤ëª°í†¡ í”Œë˜ê·¸ ì´ˆê¸°í™” (ë‹¤ìŒ ì…ë ¥ì€ ì‹¤ì œ ë¬¸ì˜ì¼ ê²ƒ)
    state["is_small_talk"] = False

    return state
