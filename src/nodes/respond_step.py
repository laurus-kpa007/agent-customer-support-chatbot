"""Respond Step Node - ë‹¨ê³„ë³„ ì‘ë‹µ

í˜„ì¬ ë‹¨ê³„ì˜ í•´ê²° ë°©ë²•ì„ ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´í•©ë‹ˆë‹¤.
"""

import sys
from pathlib import Path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from typing import Dict, Any
import os
from langchain_core.messages import AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama

from src.models.state import SupportState


def respond_step_node(state: SupportState) -> Dict[str, Any]:
    """
    í˜„ì¬ ë‹¨ê³„ì˜ ë‹µë³€ ì œê³µ
    - ì‚¬ìš©ìì—ê²Œ í˜„ì¬ ë‹¨ê³„ ì•ˆë‚´
    - Human-in-the-Loopì„ ìœ„í•œ ì‘ë‹µ ìƒì„±

    Args:
        state: í˜„ì¬ ìƒíƒœ

    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (messagesì— ì‘ë‹µ ì¶”ê°€)
    """

    current_idx = state["current_step"]
    steps = state["solution_steps"]

    # ì²« ì‘ë‹µì‹œ ê²€ìƒ‰ ê²°ê³¼ ì •ë³´ í¬í•¨
    # ì¡°ê±´: í˜„ì¬ ë‹¨ê³„ê°€ 0ì´ê³ , retrieved_docsê°€ ìˆìœ¼ë©´ (ìƒˆ ê²€ìƒ‰ ì§í›„)
    is_first_response = current_idx == 0 and state.get("retrieved_docs") and len(state.get("retrieved_docs", [])) > 0

    # í˜„ì¬ ë‹¨ê³„ê°€ ì—†ìœ¼ë©´ ì—ìŠ¤ì»¬ë ˆì´ì…˜
    if current_idx >= len(steps):
        state["status"] = "escalated"
        state["unresolved_reason"] = "ëª¨ë“  ë‹¨ê³„ë¥¼ ì‹œë„í–ˆìœ¼ë‚˜ í•´ê²°ë˜ì§€ ì•ŠìŒ"

        response_text = (
            "ğŸ˜” ë¶ˆí¸ì„ ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤.\n\n"
            "ì œì•ˆë“œë¦° ëª¨ë“  í•´ê²° ë°©ë²•ì„ ì‹œë„í•˜ì…¨ì§€ë§Œ ë¬¸ì œê°€ í•´ê²°ë˜ì§€ ì•Šì€ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n"
            "ë‹´ë‹¹ ë¶€ì„œì˜ í™•ì¸ì´ í•„ìš”í•œ ìƒí™©ì…ë‹ˆë‹¤.\n\n"
            "ğŸ’¬ **í˜„ì¬ê¹Œì§€ì˜ ë¬¸ì˜ ë‚´ìš©ìœ¼ë¡œ ë¬¸ì˜ë¥¼ ë“±ë¡í•˜ì‹œê² ìŠµë‹ˆê¹Œ?**\n"
            "(ë‹µë³€: 'ë„¤, ë“±ë¡í•´ì£¼ì„¸ìš”' ë˜ëŠ” 'ì•„ë‹ˆìš”')"
        )
    else:
        current_step = steps[current_idx]
        step_num = current_step["step"]
        total_steps = len(steps)

        # ì²« ì‘ë‹µì‹œ ê²€ìƒ‰ ì •ë³´ ì¶”ê°€
        search_info = ""
        if is_first_response and state.get("retrieved_docs"):
            docs = state["retrieved_docs"]
            search_info = f"ğŸ” **ê²€ìƒ‰ ê²°ê³¼**: {len(docs)}ê°œì˜ ê´€ë ¨ FAQë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n"
            search_info += f"ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ: **{docs[0]['title']}** (ì¹´í…Œê³ ë¦¬: {docs[0]['category']})\n\n"

        # LLM ì´ˆê¸°í™”
        llm_model = os.getenv("OLLAMA_LLM_MODEL", "gemma2:27b")
        llm = ChatOllama(
            model=llm_model,
            base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
            temperature=0.3  # ëª…í™•í•œ ì§€ì‹œë¥¼ ìœ„í•´ ë‚®ì€ ì˜¨ë„
        )

        # ë‹¨ê³„ë³„ ì‘ë‹µ í”„ë¡¬í”„íŠ¸
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ê¼¼ê¼¼í•œ ê¸°ìˆ  ì§€ì› AI ìƒë‹´ì›ì…ë‹ˆë‹¤.
            
            ì‚¬ìš©ìì—ê²Œ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì•ˆë‚´í•˜ì„¸ìš”.
            
            í˜„ì¬ ë‹¨ê³„ ì •ë³´:
            - ë‹¨ê³„ ë²ˆí˜¸: {step_num}/{total_steps}
            - ì¡°ì¹˜: {action}
            - ì„¤ëª…: {description}
            - ê¸°ëŒ€ ê²°ê³¼: {expected_result}
            
            ì§€ì¹¨:
            1. ì‚¬ìš©ìì—ê²Œ ì´ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•˜ë„ë¡ ì •ì¤‘í•˜ê²Œ ìš”ì²­í•˜ì„¸ìš”.
            2. ì„¤ëª… ë¶€ë¶„ì„ ì´í•´í•˜ê¸° ì‰½ê²Œ í’€ì–´ì„œ ì´ì•¼ê¸°í•˜ì„¸ìš”.
            3. ê¸°ëŒ€ ê²°ê³¼ë¥¼ ì–¸ê¸‰í•˜ë©° ë¬´ì—‡ì„ í™•ì¸í•´ì•¼ í•˜ëŠ”ì§€ ì•Œë ¤ì£¼ì„¸ìš”.
            4. ì´ ë‹¨ê³„ë¥¼ ì‹œë„í•œ í›„ ê²°ê³¼ë¥¼ ì•Œë ¤ë‹¬ë¼ê³ (í•´ê²°ë˜ì—ˆëŠ”ì§€, ì•ˆë˜ì—ˆëŠ”ì§€) ëª…í™•íˆ ìš”ì²­í•˜ì„¸ìš”.
            5. ì´ì „ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆë‹¤ë©´(search_info) ì°¸ê³ í•˜ì—¬ ì–¸ê¸‰í•˜ì„¸ìš”.
            
            ì–´ì¡°:
            - ê²©ë ¤í•˜ê³  ì§€ì§€í•˜ëŠ” íƒœë„
            - ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ
            - "ë‹¤ìŒê³¼ ê°™ì´ í•´ë³´ì‹œê² ì–´ìš”?", "í™•ì¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤" ë“±ì˜ ì •ì¤‘í•œ í‘œí˜„ ì‚¬ìš©
            """),
            ("user", f"ê²€ìƒ‰ ì •ë³´: {{search_info}}\n\ní˜„ì¬ ë‹¨ê³„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.")
        ])

        # LLM í˜¸ì¶œ
        chain = prompt | llm
        response = chain.invoke({
            "step_num": step_num,
            "total_steps": total_steps,
            "action": current_step['action'],
            "description": current_step['description'],
            "expected_result": current_step['expected_result'],
            "search_info": search_info
        })
        response_text = response.content

    # ì‘ë‹µ ë©”ì‹œì§€ ì¶”ê°€
    state["messages"].append(AIMessage(content=response_text))
    state["status"] = "waiting_user"

    return state
