"""Evaluate Status Node - ìƒíƒœ í‰ê°€

ì‚¬ìš©ì ì‘ë‹µì„ ë¶„ì„í•˜ì—¬ ë¬¸ì œ í•´ê²° ì—¬ë¶€ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤.
"""

import sys
from pathlib import Path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

import os
import json
from typing import Dict, Any

from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import AIMessage
from dotenv import load_dotenv

from src.models.state import SupportState
from src.utils.state_reset import reset_conversation_state

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


def evaluate_status_node(state: SupportState) -> Dict[str, Any]:
    """
    ì‚¬ìš©ì ì‘ë‹µ í‰ê°€
    - ë¬¸ì œê°€ í•´ê²°ë˜ì—ˆëŠ”ì§€ íŒë‹¨
    - ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í• ì§€ ê²°ì •

    Args:
        state: í˜„ì¬ ìƒíƒœ

    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (status ì—…ë°ì´íŠ¸)
    """

    # print(f"[Evaluate] ì‹œì‘ - current_step={state.get('current_step')}, total_steps={len(state.get('solution_steps', []))}")  # ë””ë²„ê·¸

    # LLM ì´ˆê¸°í™”
    llm_model = os.getenv("OLLAMA_LLM_MODEL", "gemma2:27b")
    llm = ChatOllama(
        model=llm_model,
        base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
        temperature=0
    )

    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ì‘ë‹µ ê°€ì ¸ì˜¤ê¸°
    last_user_message = ""
    for msg in reversed(state["messages"]):
        if msg.type == "human":
            last_user_message = msg.content
            break

    # print(f"[Evaluate] ì‚¬ìš©ì ì‘ë‹µ: {last_user_message}")  # ë””ë²„ê·¸

    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨ (ë¹ ë¥¸ ì‘ë‹µ)
    lower_msg = last_user_message.lower()

    # í•´ê²°ë¨
    if any(keyword in lower_msg for keyword in ["í•´ê²°", "ëì–´ìš”", "ëìŠµë‹ˆë‹¤", "ê°ì‚¬", "ê³ ë§ˆì›Œ"]):
        # print("[Evaluate] â†’ í•´ê²°ë¨ (resolved)")  # ë””ë²„ê·¸
        state["status"] = "resolved"
        state["messages"].append(
            AIMessage(content="ğŸ‰ ë¬¸ì œê°€ í•´ê²°ë˜ì–´ ë‹¤í–‰ì…ë‹ˆë‹¤!\n\nì¶”ê°€ë¡œ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë¬¸ì˜í•´ì£¼ì„¸ìš”. ğŸ˜Š")
        )
        # ëŒ€í™” ìƒíƒœ ì´ˆê¸°í™”
        state = reset_conversation_state(state)
        return state

    # ì—ìŠ¤ì»¬ë ˆì´ì…˜
    if any(keyword in lower_msg for keyword in ["ë“±ë¡", "ë¬¸ì˜", "í‹°ì¼“", "ìƒë‹´ì›"]):
        # print("[Evaluate] â†’ ì—ìŠ¤ì»¬ë ˆì´ì…˜ (escalated)")  # ë””ë²„ê·¸
        state["status"] = "escalated"
        state["unresolved_reason"] = "ì‚¬ìš©ìê°€ ì§ì ‘ ë¬¸ì˜ ë“±ë¡ ìš”ì²­"
        return state

    # LLMì„ ì‚¬ìš©í•œ ì •ë°€ ë¶„ì„
    current_idx = state["current_step"]
    solution_steps = state.get("solution_steps", [])

    # solution_stepsê°€ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ì—ìŠ¤ì»¬ë ˆì´ì…˜
    if not solution_steps or len(solution_steps) == 0:
        state["status"] = "escalated"
        state["unresolved_reason"] = "í•´ê²° ë‹¨ê³„ê°€ ì—†ìŒ"
        return state

    current_step = solution_steps[current_idx] if current_idx < len(solution_steps) else None

    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ê³ ê°ì§€ì› ëŒ€í™”ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì‘ë‹µì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ íŒë‹¨í•˜ì„¸ìš”:

1. "resolved": ë¬¸ì œê°€ ì™„ì „íˆ í•´ê²°ë¨ (ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ í•´ê²°ë˜ì—ˆë‹¤ê³  í•¨)
2. "waiting": ì‚¬ìš©ìê°€ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•˜ê² ë‹¤ê³  ë™ì˜í–ˆì§€ë§Œ ì•„ì§ ê²°ê³¼ë¥¼ ë³´ê³ í•˜ì§€ ì•ŠìŒ
3. "continue": í˜„ì¬ ë‹¨ê³„ ì™„ë£Œ í›„ ë‹¤ìŒ ë‹¨ê³„ í•„ìš”, ë˜ëŠ” ë¬¸ì œê°€ ì—¬ì „í•¨
4. "escalate": ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ë¬¸ì˜ ë“±ë¡/ìƒë‹´ì› ì—°ê²° ìš”ì²­

íŒë‹¨ ê¸°ì¤€:
- "í•´ê²°ëì–´ìš”", "ì´ì œ ë¼ìš”", "ì‘ë™í•©ë‹ˆë‹¤", "ê°ì‚¬í•©ë‹ˆë‹¤(í•´ê²° í›„)" ë“± â†’ resolved
- "ì•Œê² ì–´ìš”", "í•œë²ˆ í•´ë³¼ê²Œìš”", "ì‹œë„í•´ë³¼ê²Œ", "í™•ì¸í•´ë³¼ê²Œ", "í…ŒìŠ¤íŠ¸í•´ë³¼ê²Œ", "í•´ë³´ê² ìŠµë‹ˆë‹¤" ë“± â†’ waiting
    - ì£¼ì˜: ì‚¬ìš©ìê°€ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•˜ê² ë‹¤ê³  ë™ì˜ë§Œ í•œ ê²½ìš°, ì•„ì§ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë ¤ì•¼ í•©ë‹ˆë‹¤.
    - ì˜ˆ1: "ì•Œê² ì–´ í•œë²ˆ ì‹œë„í•´ë³¼ê»˜" â†’ ì•„ì§ í…ŒìŠ¤íŠ¸ ì•ˆí•¨ â†’ waiting
    - ì˜ˆ2: "í™•ì¸í•´ë³¼ê²Œìš”" â†’ ì•„ì§ í™•ì¸ ì•ˆí•¨ â†’ waiting
- "ë„¤íŠ¸ì›Œí¬ëŠ” ì •ìƒì´ì•¼", "í™•ì¸í–ˆëŠ”ë° ì•ˆë¼ìš”", "ì„¤ì •ì€ ë§ì•„ìš”", "ë‹¤ìŒ ë‹¨ê³„ ì•Œë ¤ì¤˜", "íŒŒì¼ í¬ê¸°ëŠ” ì‘ì•„ìš”" ë“± â†’ continue
    - ì£¼ì˜: ì‚¬ìš©ìê°€ í˜„ì¬ ë‹¨ê³„ì˜ ì ê²€ ì‚¬í•­ì´ 'ì •ìƒ'ì´ë¼ê³  ë§í•˜ëŠ” ê²ƒì€ ë¬¸ì œê°€ í•´ê²°ë˜ì—ˆë‹¤ëŠ” ëœ»ì´ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ì˜ˆ1: "ì¸í„°ë„·ì€ ì—°ê²°ë˜ì–´ ìˆì–´" â†’ ì¸í„°ë„· ë¬¸ì œëŠ” ì•„ë‹ˆì§€ë§Œ ì›ë˜ ë¬¸ì œëŠ” ì—¬ì „í•¨ â†’ continue
    - ì˜ˆ2: "íŒŒì¼ í¬ê¸°ëŠ” 1MBì•¼" â†’ ìš©ëŸ‰ ë¬¸ì œëŠ” ì•„ë‹ˆì§€ë§Œ ì—…ë¡œë“œ ì‹¤íŒ¨ëŠ” ì—¬ì „í•¨ â†’ continue
- "ë“±ë¡í•´ì£¼ì„¸ìš”", "ë¬¸ì˜í• ê²Œìš”", "ìƒë‹´ì› ì—°ê²°í•´ì¤˜" ë“± â†’ escalate

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{"decision": "resolved|waiting|continue|escalate", "reason": "íŒë‹¨ ì´ìœ "}}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""),
        ("user", """í˜„ì¬ ë‹¨ê³„: {current_step}
ì‚¬ìš©ì ì‘ë‹µ: {user_response}""")
    ])

    try:
        response = llm.invoke(
            prompt.format_messages(
                current_step=str(current_step) if current_step else "N/A",
                user_response=last_user_message
            )
        )

        # JSON íŒŒì‹±
        content = response.content.strip()
        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:].strip()

        evaluation = json.loads(content)
        decision = evaluation.get("decision", "continue")

        if decision == "resolved":
            # print("[Evaluate] LLM íŒë‹¨ â†’ resolved")  # ë””ë²„ê·¸
            state["status"] = "resolved"
            state["messages"].append(
                AIMessage(content="ğŸ‰ ë¬¸ì œê°€ í•´ê²°ë˜ì–´ ë‹¤í–‰ì…ë‹ˆë‹¤!\n\nì¶”ê°€ë¡œ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë¬¸ì˜í•´ì£¼ì„¸ìš”. ğŸ˜Š")
            )
            # ëŒ€í™” ìƒíƒœ ì´ˆê¸°í™”
            state = reset_conversation_state(state)
        elif decision == "waiting":
            # print("[Evaluate] LLM íŒë‹¨ â†’ waiting")  # ë””ë²„ê·¸
            # ì‚¬ìš©ìê°€ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•˜ê² ë‹¤ê³  ë™ì˜í–ˆì§€ë§Œ ì•„ì§ ê²°ê³¼ë¥¼ ë³´ê³ í•˜ì§€ ì•ŠìŒ
            # ê°™ì€ ë‹¨ê³„ë¥¼ ìœ ì§€í•˜ê³  ì‚¬ìš©ì ì‘ë‹µ ëŒ€ê¸°
            state["status"] = "waiting_user"
            state["messages"].append(
                AIMessage(content="ë„¤, í™•ì¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤. ê²°ê³¼ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì•ˆë‚´í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ğŸ˜Š")
            )
        elif decision == "escalate":
            # print("[Evaluate] LLM íŒë‹¨ â†’ escalate")  # ë””ë²„ê·¸
            state["status"] = "escalated"
            state["unresolved_reason"] = evaluation.get("reason", "ì‚¬ìš©ì ìš”ì²­")
        else:  # continue
            # print(f"[Evaluate] LLM íŒë‹¨ â†’ continue (step {current_idx} â†’ {current_idx + 1})")  # ë””ë²„ê·¸
            # í˜„ì¬ ë‹¨ê³„ë¥¼ ì™„ë£Œë¡œ í‘œì‹œí•˜ê³  ë‹¤ìŒ ë‹¨ê³„ë¡œ
            if current_step:
                current_step["completed"] = True
            state["current_step"] += 1
            state["status"] = "responding"

    except (json.JSONDecodeError, Exception) as e:
        # ê¸°ë³¸ ë™ì‘: ë‹¤ìŒ ë‹¨ê³„ë¡œ
        # print(f"[Evaluate] Warning: í‰ê°€ ì‹¤íŒ¨, ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰: {e}")  # ë””ë²„ê·¸
        if current_step:
            current_step["completed"] = True
        state["current_step"] += 1
        state["status"] = "responding"

    # print(f"[Evaluate] ì™„ë£Œ - status={state['status']}, current_step={state.get('current_step')}")  # ë””ë²„ê·¸
    return state
