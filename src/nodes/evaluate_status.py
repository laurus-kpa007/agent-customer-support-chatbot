"""Evaluate Status Node - ìƒíƒœ í‰ê°€

ì‚¬ìš©ì ì‘ë‹µì„ ë¶„ì„í•˜ì—¬ ë¬¸ì œ í•´ê²° ì—¬ë¶€ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤.
"""

import sys
from pathlib import Path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

import os
import json
from typing import Dict, Any

from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import AIMessage
from dotenv import load_dotenv

from src.models.state import SupportState

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


def evaluate_status_node(state: SupportState) -> Dict[str, Any]:
    """
    ì‚¬ìš©ì ì‘ë‹µ í‰ê°€
    - ë¬¸ì œê°€ í•´ê²°ë˜ì—ˆëŠ”ì§€ íŒë‹¨
    - ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í• ì§€ ê²°ì •

    Args:
        state: í˜„ì¬ ìƒíƒœ

    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (status ì—…ë°ì´íŠ¸)
    """

    # print(f"[Evaluate] ì‹œì‘ - current_step={state.get('current_step')}, total_steps={len(state.get('solution_steps', []))}")  # ë””ë²„ê·¸

    # LLM ì´ˆê¸°í™”
    llm_model = os.getenv("OLLAMA_LLM_MODEL", "gemma2:27b")
    llm = ChatOllama(
        model=llm_model,
        base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
        temperature=0
    )

    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ì‘ë‹µ ê°€ì ¸ì˜¤ê¸°
    last_user_message = ""
    for msg in reversed(state["messages"]):
        if msg.type == "human":
            last_user_message = msg.content
            break

    # print(f"[Evaluate] ì‚¬ìš©ì ì‘ë‹µ: {last_user_message}")  # ë””ë²„ê·¸

    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨ (ë¹ ë¥¸ ì‘ë‹µ)
    lower_msg = last_user_message.lower()

    # í•´ê²°ë¨
    if any(keyword in lower_msg for keyword in ["í•´ê²°", "ëì–´ìš”", "ëìŠµë‹ˆë‹¤", "ê°ì‚¬", "ê³ ë§ˆì›Œ"]):
        # print("[Evaluate] â†’ í•´ê²°ë¨ (resolved)")  # ë””ë²„ê·¸
        state["status"] = "resolved"
        state["messages"].append(
            AIMessage(content="ğŸ‰ ë¬¸ì œê°€ í•´ê²°ë˜ì–´ ë‹¤í–‰ì…ë‹ˆë‹¤!\n\nì¶”ê°€ë¡œ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë¬¸ì˜í•´ì£¼ì„¸ìš”. ğŸ˜Š")
        )
        return state

    # ì—ìŠ¤ì»¬ë ˆì´ì…˜
    if any(keyword in lower_msg for keyword in ["ë“±ë¡", "ë¬¸ì˜", "í‹°ì¼“", "ìƒë‹´ì›"]):
        # print("[Evaluate] â†’ ì—ìŠ¤ì»¬ë ˆì´ì…˜ (escalated)")  # ë””ë²„ê·¸
        state["status"] = "escalated"
        state["unresolved_reason"] = "ì‚¬ìš©ìê°€ ì§ì ‘ ë¬¸ì˜ ë“±ë¡ ìš”ì²­"
        return state

    # LLMì„ ì‚¬ìš©í•œ ì •ë°€ ë¶„ì„
    current_idx = state["current_step"]
    solution_steps = state.get("solution_steps", [])

    # solution_stepsê°€ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ì—ìŠ¤ì»¬ë ˆì´ì…˜
    if not solution_steps or len(solution_steps) == 0:
        state["status"] = "escalated"
        state["unresolved_reason"] = "í•´ê²° ë‹¨ê³„ê°€ ì—†ìŒ"
        return state

    current_step = solution_steps[current_idx] if current_idx < len(solution_steps) else None

    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ê³ ê°ì§€ì› ëŒ€í™”ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì‘ë‹µì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ íŒë‹¨í•˜ì„¸ìš”:

1. "resolved": ë¬¸ì œê°€ í•´ê²°ë¨
2. "continue": í˜„ì¬ ë‹¨ê³„ê°€ íš¨ê³¼ ì—†ìŒ, ë‹¤ìŒ ë‹¨ê³„ í•„ìš”
3. "escalate": ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ë¬¸ì˜ ë“±ë¡ ìš”ì²­

íŒë‹¨ ê¸°ì¤€:
- "í•´ê²°ëì–´ìš”", "ëì–´ìš”", "ê°ì‚¬í•©ë‹ˆë‹¤" ë“± â†’ resolved
- "ì•ˆë¼ìš”", "ì—¬ì „íˆ", "ì²´í¬ë˜ì–´ ìˆëŠ”ë°", "ì•ˆ ë©ë‹ˆë‹¤" ë“± â†’ continue
- "ë“±ë¡í•´ì£¼ì„¸ìš”", "ë¬¸ì˜í• ê²Œìš”", "ìƒë‹´ì›" ë“± â†’ escalate

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{"decision": "resolved|continue|escalate", "reason": "íŒë‹¨ ì´ìœ "}}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""),
        ("user", """í˜„ì¬ ë‹¨ê³„: {current_step}
ì‚¬ìš©ì ì‘ë‹µ: {user_response}""")
    ])

    try:
        response = llm.invoke(
            prompt.format_messages(
                current_step=str(current_step) if current_step else "N/A",
                user_response=last_user_message
            )
        )

        # JSON íŒŒì‹±
        content = response.content.strip()
        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:].strip()

        evaluation = json.loads(content)
        decision = evaluation.get("decision", "continue")

        if decision == "resolved":
            # print("[Evaluate] LLM íŒë‹¨ â†’ resolved")  # ë””ë²„ê·¸
            state["status"] = "resolved"
            state["messages"].append(
                AIMessage(content="ğŸ‰ ë¬¸ì œê°€ í•´ê²°ë˜ì–´ ë‹¤í–‰ì…ë‹ˆë‹¤!\n\nì¶”ê°€ë¡œ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë¬¸ì˜í•´ì£¼ì„¸ìš”. ğŸ˜Š")
            )
        elif decision == "escalate":
            # print("[Evaluate] LLM íŒë‹¨ â†’ escalate")  # ë””ë²„ê·¸
            state["status"] = "escalated"
            state["unresolved_reason"] = evaluation.get("reason", "ì‚¬ìš©ì ìš”ì²­")
        else:  # continue
            # print(f"[Evaluate] LLM íŒë‹¨ â†’ continue (step {current_idx} â†’ {current_idx + 1})")  # ë””ë²„ê·¸
            # í˜„ì¬ ë‹¨ê³„ë¥¼ ì™„ë£Œë¡œ í‘œì‹œí•˜ê³  ë‹¤ìŒ ë‹¨ê³„ë¡œ
            if current_step:
                current_step["completed"] = True
            state["current_step"] += 1
            state["status"] = "responding"

    except (json.JSONDecodeError, Exception) as e:
        # ê¸°ë³¸ ë™ì‘: ë‹¤ìŒ ë‹¨ê³„ë¡œ
        # print(f"[Evaluate] Warning: í‰ê°€ ì‹¤íŒ¨, ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰: {e}")  # ë””ë²„ê·¸
        if current_step:
            current_step["completed"] = True
        state["current_step"] += 1
        state["status"] = "responding"

    # print(f"[Evaluate] ì™„ë£Œ - status={state['status']}, current_step={state.get('current_step')}")  # ë””ë²„ê·¸
    return state
